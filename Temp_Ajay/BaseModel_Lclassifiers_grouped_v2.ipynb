{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIJGTIRqwShE",
    "outputId": "3f3e80b7-e24f-488a-d435-f006a65a3eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: packaging in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: requests in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7SvCi2JuhC5P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay Narasimha\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def write_tsv_dataframe(filepath, dataframe):\n",
    "    \"\"\"\n",
    "        Stores `DataFrame` as tsv file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filepath : str\n",
    "            Path to tsv file\n",
    "        dataframe : pd.DataFrame\n",
    "            DataFrame to store\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        IOError\n",
    "            if the file can't be opened\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataframe.to_csv(filepath, encoding='utf-8', sep='\\t', index=False, header=True, quoting=csv.QUOTE_NONE)\n",
    "    except IOError:\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ve5zjHvhhIMT"
   },
   "outputs": [],
   "source": [
    "def combine_columns(df_arguments, df_labels):\n",
    "    \"\"\"Combines the two `DataFrames` on column `Argument ID`\"\"\"\n",
    "    return pd.merge(df_arguments, df_labels, on='Argument ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tnaaQHrnhYnh"
   },
   "outputs": [],
   "source": [
    "def split_arguments(df_arguments):\n",
    "    \"\"\"Splits `DataFrame` by column `Usage` into `train`-, `validation`-, and `test`-arguments\"\"\"\n",
    "    train_arguments = df_arguments.loc[df_arguments['Usage'] == 'train'].drop(['Usage'], axis=1).reset_index(drop=True)\n",
    "    valid_arguments = df_arguments.loc[df_arguments['Usage'] == 'validation'].drop(['Usage'], axis=1).reset_index(drop=True)\n",
    "    test_arguments = df_arguments.loc[df_arguments['Usage'] == 'test'].drop(['Usage'], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    return train_arguments, valid_arguments, test_arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kpljNrsChbf1"
   },
   "outputs": [],
   "source": [
    "def create_dataframe_head(argument_ids, model_name):\n",
    "    \"\"\"\n",
    "        Creates `DataFrame` usable to append predictions to it\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        argument_ids : list[str]\n",
    "            First column of the resulting DataFrame\n",
    "        model_name : str\n",
    "            Second column of DataFrame will contain the given model name\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            prepared DataFrame\n",
    "    \"\"\"\n",
    "    df_model_head = pd.DataFrame(argument_ids, columns=['Argument ID'])\n",
    "    df_model_head['Method'] = [model_name] * len(argument_ids)\n",
    "\n",
    "    return df_model_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1MgX3B55hd9P"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "class MissingColumnError(AttributeError):\n",
    "    \"\"\"Error indicating that an imported DataFrame lacks necessary columns\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Qe-7L7j3ho_X"
   },
   "outputs": [],
   "source": [
    "def load_json_file(filepath):\n",
    "    \"\"\"Load content of json-file from `filepath`\"\"\"\n",
    "    with open(filepath, 'r') as  json_file:\n",
    "        return json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vWGzjs9lhq_s"
   },
   "outputs": [],
   "source": [
    "def load_values_from_json(filepath):\n",
    "    \"\"\"Load values per level from json-file from `filepath`\"\"\"\n",
    "    json_values = load_json_file(filepath)\n",
    "    values = { \"1\":set(), \"2\":set(), \"3\":set(), \"4a\":set(), \"4b\":set() }\n",
    "    for value in json_values[\"values\"]:\n",
    "        values[\"1\"].add(value[\"name\"])\n",
    "        values[\"2\"].add(value[\"level2\"])\n",
    "        for valueLevel3 in value[\"level3\"]:\n",
    "            values[\"3\"].add(valueLevel3)\n",
    "        for valueLevel4a in value[\"level4a\"]:\n",
    "            values[\"4a\"].add(valueLevel4a)\n",
    "        for valueLevel4b in value[\"level4b\"]:\n",
    "            values[\"4b\"].add(valueLevel4b)\n",
    "    values[\"1\"] = sorted(values[\"1\"])\n",
    "    values[\"2\"] = sorted(values[\"2\"])\n",
    "    values[\"3\"] = sorted(values[\"3\"])\n",
    "    values[\"4a\"] = sorted(values[\"4a\"])\n",
    "    values[\"4b\"] = sorted(values[\"4b\"])\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HpBKjzIihtk0"
   },
   "outputs": [],
   "source": [
    "def load_arguments_from_tsv(filepath, default_usage='test'):\n",
    "    \"\"\"\n",
    "        Reads arguments from tsv file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filepath : str\n",
    "            The path to the tsv file\n",
    "        default_usage : str, optional\n",
    "            The default value if the column \"Usage\" is missing\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            the DataFrame with all arguments\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        MissingColumnError\n",
    "            if the required columns \"Argument ID\" or \"Premise\" are missing in the read data\n",
    "        IOError\n",
    "            if the file can't be read\n",
    "        \"\"\"\n",
    "    try:\n",
    "        dataframe = pd.read_csv(filepath, encoding='utf-8', sep='\\t', header=0)\n",
    "        if not {'Argument ID', 'Premise'}.issubset(set(dataframe.columns.values)):\n",
    "            raise MissingColumnError('The argument \"%s\" file does not contain the minimum required columns [Argument ID, Premise].' % filepath)\n",
    "        if 'Usage' not in dataframe.columns.values:\n",
    "            dataframe['Usage'] = [default_usage] * len(dataframe)\n",
    "        return dataframe\n",
    "    except IOError:\n",
    "        traceback.print_exc()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lTFyn21VhwsY"
   },
   "outputs": [],
   "source": [
    "def load_labels_from_tsv(filepath, label_order):\n",
    "    \"\"\"\n",
    "        Reads label annotations from tsv file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filepath : str\n",
    "            The path to the tsv file\n",
    "        label_order : list[str]\n",
    "            The listing and order of the labels to use from the read data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            the DataFrame with the annotations\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        MissingColumnError\n",
    "            if the required columns \"Argument ID\" or names from `label_order` are missing in the read data\n",
    "        IOError\n",
    "            if the file can't be read\n",
    "        \"\"\"\n",
    "    try:\n",
    "        dataframe = pd.read_csv(filepath, encoding='utf-8', sep='\\t', header=0)\n",
    "        dataframe = dataframe[['Argument ID'] + label_order]\n",
    "        return dataframe\n",
    "    except IOError:\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    except KeyError:\n",
    "        raise MissingColumnError('The file \"%s\" does not contain the required columns for its level.' % filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yxSNyT8Why1g"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import getopt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dQ_Eh4qMi6dA"
   },
   "outputs": [],
   "source": [
    "model_dir = 'models'\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HHejuM1kh4i8"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "eyHY5YMAi5D9"
   },
   "outputs": [],
   "source": [
    "argument_filepath = os.path.join(data_dir, 'arguments.tsv')\n",
    "value_json_filepath = os.path.join(data_dir, 'values.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "cNk-VDCUjPh1"
   },
   "outputs": [],
   "source": [
    "df_arguments = load_arguments_from_tsv(argument_filepath, default_usage='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "cN1Ugt6xjUla"
   },
   "outputs": [],
   "source": [
    "values = load_values_from_json(value_json_filepath)\n",
    "num_labels_Lv2 = len(values['2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sZzSzdQmYi7",
    "outputId": "feca34dc-04db-4aac-8359-4aec5ed73c20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Argument ID', 'Part', 'Usage', 'Conclusion', 'Stance', 'Premise'], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arguments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gRHUX8b_n9ve"
   },
   "outputs": [],
   "source": [
    "# for ip in df_arguments['Argument ID']:\n",
    "#   #print(df_arguments['Stance'][ip])\n",
    "#   print(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "b-L7fgEjnXGK"
   },
   "outputs": [],
   "source": [
    "level =2\n",
    "label_filepath = os.path.join(data_dir, 'labels-level{}.tsv'.format(str(level)))\n",
    "df_labels = load_labels_from_tsv(label_filepath, values[str(level)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIBMeZIfntMM",
    "outputId": "df5ebd58-aadb-4463-8e58-f30d40929552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5270 Argument ID\n",
      "5270 Achievement\n",
      "5270 Benevolence: caring\n",
      "5270 Benevolence: dependability\n",
      "5270 Conformity: interpersonal\n",
      "5270 Conformity: rules\n",
      "5270 Face\n",
      "5270 Hedonism\n",
      "5270 Humility\n",
      "5270 Power: dominance\n",
      "5270 Power: resources\n",
      "5270 Security: personal\n",
      "5270 Security: societal\n",
      "5270 Self-direction: action\n",
      "5270 Self-direction: thought\n",
      "5270 Stimulation\n",
      "5270 Tradition\n",
      "5270 Universalism: concern\n",
      "5270 Universalism: nature\n",
      "5270 Universalism: objectivity\n",
      "5270 Universalism: tolerance\n"
     ]
    }
   ],
   "source": [
    "a = df_labels.keys()\n",
    "for key in df_labels.keys():\n",
    "  print(len(df_labels[key]),key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7ONfVdGtsQ5",
    "outputId": "51cb5577-e2eb-443f-c32b-61a20640873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels['Achievement'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MurNX18XjWIw"
   },
   "outputs": [],
   "source": [
    "# from typing import Dict, List\n",
    "# #def generate_pairwise_input(dataset: Dict[List], labels: Dict[List]) -> (List[str], List[str], List[str], List[int]):\n",
    "# def generate_pairwise_input(dataset, labels):\n",
    "#     \"\"\"\n",
    "#     TODO: group all premises and corresponding hypotheses and labels of the datapoints\n",
    "#     a datapoint as seen earlier is a dict of premis, hypothesis and label\n",
    "#     \"\"\"\n",
    "#     #raise NotImplementedError\n",
    "#     premise=[]\n",
    "#     conclusion=[]\n",
    "#     stance=[]\n",
    "#     n_labels =labels.keys()\n",
    "#     n_labels = n_labels[1:]\n",
    "#     print(n_labels)\n",
    "#     label=[]\n",
    "    \n",
    "#     n = len(dataset['Argument ID'])\n",
    "#     m = len(labels['Argument ID'])\n",
    "#     print(n,m)\n",
    "#     for i in range(n):\n",
    "#         premise.append(dataset['Premise'][i])\n",
    "#         conclusion.append(dataset['Conclusion'][i])\n",
    "#         stance.append(dataset['Stance'][i])\n",
    "#     for l in range(len(n_labels)):\n",
    "#         label_id = []\n",
    "#         #print(i)\n",
    "#         for i in range(m):\n",
    "#             #print(n_labels[l])\n",
    "#             label_id.append(int(labels[n_labels[l]][i]))\n",
    "#         label.append(label_id)\n",
    "\n",
    "#     return premise, conclusion, stance, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "QoizFWHAVzgp"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "#def generate_pairwise_input(dataset: Dict[List], labels: Dict[List]) -> (List[str], List[str], List[str], List[int]):\n",
    "def generate_pairwise_input(dataset, labels):\n",
    "    \"\"\"\n",
    "    TODO: group all premises and corresponding hypotheses and labels of the datapoints\n",
    "    a datapoint as seen earlier is a dict of premis, hypothesis and label\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "    premise=[]\n",
    "    conclusion=[]\n",
    "    stance=[]\n",
    "    n_labels =labels.keys()\n",
    "    n_labels = n_labels[1:]\n",
    "    print(n_labels)\n",
    "    label=[]\n",
    "    \n",
    "    n = len(dataset['Argument ID'])\n",
    "    m = len(labels['Argument ID'])\n",
    "    print(n,m)\n",
    "    for i in range(n):\n",
    "        premise.append(dataset['Premise'][i])\n",
    "        conclusion.append(dataset['Conclusion'][i])\n",
    "        stance.append(dataset['Stance'][i])\n",
    "    for i in range(m):\n",
    "        sent_label = []\n",
    "        #print(i)\n",
    "        for l in range(len(n_labels)):\n",
    "            #print(n_labels[l])\n",
    "            sent_label.append(int(labels[n_labels[l]][i]))\n",
    "        label.append(sent_label)\n",
    "\n",
    "    return premise, conclusion, stance, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBU9bpZwjvH9",
    "outputId": "f9345802-0af1-4d90-9b03-765e58fb52df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Achievement', 'Benevolence: caring', 'Benevolence: dependability',\n",
      "       'Conformity: interpersonal', 'Conformity: rules', 'Face', 'Hedonism',\n",
      "       'Humility', 'Power: dominance', 'Power: resources',\n",
      "       'Security: personal', 'Security: societal', 'Self-direction: action',\n",
      "       'Self-direction: thought', 'Stimulation', 'Tradition',\n",
      "       'Universalism: concern', 'Universalism: nature',\n",
      "       'Universalism: objectivity', 'Universalism: tolerance'],\n",
      "      dtype='object')\n",
      "5270 5270\n"
     ]
    }
   ],
   "source": [
    "#Randomize them first\n",
    "train_premises, train_conclusion, train_stance, train_labels = generate_pairwise_input(df_arguments, df_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "nKP166RxYrGw"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "def randomize_data(premises, conclusion, stance, labels):\n",
    "  n = len(premises)\n",
    "  data = list(range(n))\n",
    "  random.shuffle(data)\n",
    "  train_premises = []\n",
    "  train_conclusion = []\n",
    "  train_stance = []\n",
    "  train_labels = []\n",
    "  for i in data:\n",
    "    train_premises.append(premises[i])\n",
    "    train_conclusion.append(conclusion[i])\n",
    "    train_stance.append(stance[i])\n",
    "    train_labels.append(labels[:][i])\n",
    "  return train_premises, train_conclusion, train_stance, train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "N60nWP8mUib_"
   },
   "outputs": [],
   "source": [
    "# a = train_labels[:][-5:]\n",
    "# len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "0Ye8wftgaawi"
   },
   "outputs": [],
   "source": [
    "train_premises, train_conclusion, train_stance, train_labels = randomize_data(train_premises, train_conclusion, train_stance, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "o3ygsKZZzBhs"
   },
   "outputs": [],
   "source": [
    "val_premises = train_premises[-500:]\n",
    "val_conclusion = train_conclusion[-500:]\n",
    "val_stance = train_stance[-500:]\n",
    "val_labels = train_labels[:][-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "KveGMjWxzpFF"
   },
   "outputs": [],
   "source": [
    "train_premises = train_premises[:-500]\n",
    "train_conclusion = train_conclusion[:-500]\n",
    "train_stance = train_stance[:-500]\n",
    "train_labels = train_labels[:][:-500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhW38nWHv2tj",
    "outputId": "c7c149ac-8c52-4a68-f2bb-d282d16e9d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  1996, 18458,  1012,   102,  1999,  7927,  1997,\n",
      "          2023,  2003,  1996, 10744,   102],\n",
      "        [  101,  2023,  2003,  2036,  1037, 18458,   102,  2114,  2023,  2003,\n",
      "          1037,  2117, 10744,   102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] this is the premise. [SEP] in favour of this is the hypothesis [SEP]',\n",
       " '[CLS] this is also a premise [SEP] against this is a second hypothesis [SEP] [PAD]']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nothing to do for this class!\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "\n",
    "class BatchTokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the tokenizer\n",
    "\n",
    "        Args:\n",
    "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\n",
    "        \"\"\"\n",
    "        self.hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    def get_sep_token(self,):\n",
    "        return self.hf_tokenizer.sep_token\n",
    "    \n",
    "    def __call__(self, prem_batch: List[str], hyp_batch: List[str], stance_batch: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\n",
    "\n",
    "        We return a dictionary of tensors per the huggingface model specification.\n",
    "\n",
    "        Args:\n",
    "            batch (List[str]): A List of sentence strings\n",
    "\n",
    "        Returns:\n",
    "            Dict: The dictionary of token specifications provided by HuggingFace\n",
    "        \"\"\"\n",
    "        # The HF tokenizer will PAD for us, and additionally combine \n",
    "        # The two sentences deimited by the [SEP] token.\n",
    "        batch_len = len(prem_batch)\n",
    "        #spaces = [\" \"]*batch_len\n",
    "        conc_batch = [stance_batch[i]+\" \"+hyp_batch[i] for i in range(batch_len)]\n",
    "        enc = self.hf_tokenizer(\n",
    "            prem_batch,\n",
    "            conc_batch,\n",
    "            padding=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return enc\n",
    "    \n",
    "\n",
    "# HERE IS AN EXAMPLE OF HOW TO USE THE BATCH TOKENIZER\n",
    "tokenizer = BatchTokenizer()\n",
    "a = [[\"this is the premise.\", \"This is also a premise\"], [\"this is the hypothesis\", \"This is a second hypothesis\"],[\"in favour of\", \"against\"]]\n",
    "x = tokenizer(*a)\n",
    "print(x)\n",
    "tokenizer.hf_tokenizer.batch_decode(x[\"input_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "FQWwRKXgrYRY"
   },
   "outputs": [],
   "source": [
    "def chunk(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[:][i:i + n]\n",
    "\n",
    "def chunk_multi(lst1, lst2, lst3, n):\n",
    "    for i in range(0, len(lst1), n):\n",
    "        yield lst1[i: i + n], lst2[i: i + n], lst3[i: i + n]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCdddSnCwwmA",
    "outputId": "41520686-e453-40ce-aaba-e8129f6161c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16277\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "import numpy as np\n",
    "# for i in range(5270):\n",
    "#   sum += np.sum(np.array(train_labels[:][i]))\n",
    "print(np.sum(np.array(train_labels)))\n",
    "#print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "nchPyT3yAtW3"
   },
   "outputs": [],
   "source": [
    "against=0\n",
    "infavour = 0\n",
    "for i in range(4770):\n",
    "  if(train_stance[i]=='against'):\n",
    "    against +=1\n",
    "  elif(train_stance[i]=='in favor of'):\n",
    "    infavour += 1\n",
    "  else:\n",
    "    print(train_stance[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "v-COBdmXv1Gf"
   },
   "outputs": [],
   "source": [
    "# Notice that since we use huggingface, we tokenize and\n",
    "# encode in all at once!\n",
    "batch_size=64\n",
    "tokenizer = BatchTokenizer()\n",
    "train_input_batches = [b for b in chunk_multi(train_premises, train_conclusion, train_stance, batch_size)]\n",
    "# Tokenize + encode\n",
    "train_input_batches = [tokenizer(*batch) for batch in train_input_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-H-JWr6-snT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "W_ta_vSAx4oz"
   },
   "outputs": [],
   "source": [
    "val_input_batches = [b for b in chunk_multi(val_premises, val_conclusion, val_stance, batch_size)]\n",
    "# Tokenize + encode\n",
    "val_input_batches = [tokenizer(*batch) for batch in val_input_batches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VwgQMkk1ZxW",
    "outputId": "18f0ff1c-71c0-4016-e480-b9252bb0abfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "XgY4RO1-05Tz"
   },
   "outputs": [],
   "source": [
    "def encode_labels(labels: List[List[int]]) -> List[torch.FloatTensor]:\n",
    "    \"\"\"Turns the batch of labels into a tensor\n",
    "\n",
    "    Args:\n",
    "        labels (List[List[int]]): List of all labels in the batch\n",
    "\n",
    "    Returns:\n",
    "        List[torch.FloatTensor]: List of Tensors of all labels in the batch\n",
    "    \"\"\"\n",
    "    \n",
    "    return torch.LongTensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M67nEmKFR1ar",
    "outputId": "bd9c6a23-8cc5-44d0-c9c8-77732714b00c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4770 500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels), len(val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_labels(labels):\n",
    "    #JUst Grouping 17,18,19,20\n",
    "    extended_labels = []\n",
    "    for i in range(len(labels)):\n",
    "        elabels = labels[i]\n",
    "        #Add group labels\n",
    "        if (elabels[16]==1 or elabels[17]==1 or elabels[18]==1 or elabels[19]==1):\n",
    "            elabels.append(1)\n",
    "        else:\n",
    "            elabels.append(0)\n",
    "        extended_labels.append(elabels)\n",
    "    return extended_labels\n",
    "train_labels = group_labels(train_labels)\n",
    "val_labels = group_labels(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4770 500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels), len(val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "cw7wSXAk19YY"
   },
   "outputs": [],
   "source": [
    "train_label_batches = [b for b in chunk(train_labels, batch_size)]\n",
    "train_label_batches = [encode_labels(batch) for batch in train_label_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "s2Jl56Mg2HdF"
   },
   "outputs": [],
   "source": [
    "val_label_batches = [b for b in chunk(val_labels, batch_size)]\n",
    "val_label_batches = [encode_labels(batch) for batch in val_label_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9i5nn1qT244",
    "outputId": "b00cb0b8-f00e-470f-bbaa-66d44330286d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_label_batches[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "qsgi7io72ewY"
   },
   "outputs": [],
   "source": [
    "class NLIClassifier(torch.nn.Module):\n",
    "    def __init__(self, output_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize BERT, which we use instead of a single embedding layer.\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        # TODO [OPTIONAL]: Updating all BERT parameters can be slow and memory intensive. \n",
    "        # Freeze them if training is too slow. Notice that the learning\n",
    "        # rate should probably be smaller in this case.\n",
    "        # Uncommenting out the below 2 lines means only our classification layer will be updated.\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.bert_hidden_dimension = self.bert.config.hidden_size\n",
    "        print(self.bert_hidden_dimension)\n",
    "        # TODO: Add an extra hidden layer in the classifier, projecting\n",
    "        #      from the BERT hidden dimension to hidden size.\n",
    "        # TODO: Add a relu nonlinearity to be used in the forward method\n",
    "        #      https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "        self.middle_layer1 = torch.nn.Linear(self.bert_hidden_dimension, 64)\n",
    "        self.middle_layer2 = torch.nn.Linear(self.bert_hidden_dimension, 64)\n",
    "        self.middle_layer3 = torch.nn.Linear(self.bert_hidden_dimension, 64)\n",
    "        self.middle_layer4 = torch.nn.Linear(self.bert_hidden_dimension, 64)\n",
    "        self.middle_layer5 = torch.nn.Linear(self.bert_hidden_dimension, 64)\n",
    "        self.middle_layer6 = torch.nn.Linear(self.bert_hidden_dimension, 64)\n",
    "        \n",
    "        \n",
    "        self.hidden_layer1 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer2 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer3 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "        self.hidden_layer4 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "        self.hidden_layer5 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "        self.hidden_layer6 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer7 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer8 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "        self.hidden_layer9 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer10 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer11 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "        self.hidden_layer12 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer13 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer14 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "        self.hidden_layer15 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer16 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer17 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer18 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer19 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layer20 = torch.nn.Linear(64, 32)\n",
    "        self.hidden_layers = [torch.nn.Linear(self.bert_hidden_dimension, 32).to(device) for i in range(self.output_size)]\n",
    "        self.classifiers = [torch.nn.Linear(32, 1).to(device) for i in range(self.output_size)]\n",
    "        #self.hidden_layer2 = torch.nn.Linear(self.hidden_size, 32)\n",
    "        #self.hidden_layer3 = torch.nn.Linear(128, 32)\n",
    "        #self.hidden_layer4 = torch.nn.Linear(32, 8)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.classifier1 = torch.nn.Linear(32, 1)\n",
    "        self.classifier2 = torch.nn.Linear(32, 1)\n",
    "        self.classifier3 = torch.nn.Linear(32, 1)\n",
    "        self.classifier4 = torch.nn.Linear(32, 1)\n",
    "        self.classifier5 = torch.nn.Linear(32, 1)\n",
    "        self.classifier6 = torch.nn.Linear(32, 1)\n",
    "        self.classifier7 = torch.nn.Linear(32, 1)\n",
    "        self.classifier8 = torch.nn.Linear(32, 1)\n",
    "        self.classifier9 = torch.nn.Linear(32, 1)\n",
    "        self.classifier10 = torch.nn.Linear(32, 1)\n",
    "        self.classifier11 = torch.nn.Linear(32, 1)\n",
    "        self.classifier12 = torch.nn.Linear(32, 1)\n",
    "        self.classifier13 = torch.nn.Linear(32, 1)\n",
    "        self.classifier14 = torch.nn.Linear(32, 1)\n",
    "        self.classifier15 = torch.nn.Linear(32, 1)\n",
    "        self.classifier16 = torch.nn.Linear(32, 1)\n",
    "        self.classifier17 = torch.nn.Linear(32, 1)\n",
    "        self.classifier18 = torch.nn.Linear(32, 1)\n",
    "        self.classifier19 = torch.nn.Linear(32, 1)\n",
    "        self.classifier20 = torch.nn.Linear(32, 1)\n",
    "        \n",
    "        self.classifier_middle6 = torch.nn.Linear(64, 1)\n",
    "        #self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def encode_text(\n",
    "        self,\n",
    "        symbols: Dict\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
    "            Then, get the last (non-padded) hidden state for each symbol and return that.\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
    "                the entire sentence\n",
    "        \"\"\"\n",
    "        # First we get the contextualized embedding for each input symbol\n",
    "        # We no longer need an LSTM, since BERT encodes context and \n",
    "        # gives us a single vector describing the sequence in the form of the [CLS] token.\n",
    "        embedded = self.bert(**symbols)\n",
    "        #print(embedded)\n",
    "        #print(\"Embedded\", embedded.pooler_output.shape, embedded.last_hidden_state.shape)\n",
    "        # TODO: Get the [CLS] token using the `pooler_output` from \n",
    "        #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "        #      and check the returns for the forward method.\n",
    "        # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
    "        #raise NotImplementedError\n",
    "        \n",
    "        #pool_output_shape = embedded.pooler_output.shape\n",
    "        #return torch.reshape(embedded.pooler_output,(pool_output_shape[0],1,pool_output_shape[1]) )\n",
    "        last_hidden_state = embedded.last_hidden_state[:,0,:]\n",
    "        hidden_shape = last_hidden_state.shape\n",
    "        return torch.reshape(last_hidden_state,(hidden_shape[0],1,hidden_shape[1]) )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        symbols: Dict,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        #output = self.hidden_layer1(encoded_sents)\n",
    "        #output = self.relu(output)\n",
    "        #outputs = [self.hidden_layers[i](encoded_sents) for i in range(self.output_size)]\n",
    "        #outputs = [self.relu(outputs[i].to(device)) for i in range(self.output_size)]\n",
    "        #outputs = [self.classifiers[i](outputs[i].to(device)) for i in range(self.output_size)]\n",
    "        # outputs = []\n",
    "        # for i in range(self.output_size):\n",
    "        #     output = self.hidden_layers[i](encoded_sents)\n",
    "        #     output = self.relu(output)\n",
    "        #     output = self.classifiers[i](output)\n",
    "        #     output = torch.nn.Sigmoid()(output)\n",
    "        #     outputs.append(output)\n",
    "        \n",
    "        middle = self.middle_layer1(encoded_sents)\n",
    "        middle = self.relu(middle)\n",
    "        \n",
    "        #Self-direction: thought\n",
    "        output = self.hidden_layer1(middle)\n",
    "        output = self.relu(output)\n",
    "        output = self.classifier1(output)\n",
    "        output = torch.nn.Sigmoid()(output)\n",
    "        \n",
    "        #Self-direction: action\n",
    "        output2 = self.hidden_layer2(middle)\n",
    "        output2 = self.relu(output2)\n",
    "        output2 = self.classifier2(output2)\n",
    "        output2 = torch.nn.Sigmoid()(output2)\n",
    "        \n",
    "        #Stimulation\n",
    "        output3 = self.hidden_layer3(encoded_sents)\n",
    "        output3 = self.relu(output3)\n",
    "        output3 = self.classifier1(output3)\n",
    "        output3 = torch.nn.Sigmoid()(output3)\n",
    "        \n",
    "        #Hedonism\n",
    "        output4 = self.hidden_layer4(encoded_sents)\n",
    "        output4 = self.relu(output4)\n",
    "        output4 = self.classifier4(output4)\n",
    "        output4 = torch.nn.Sigmoid()(output4)\n",
    "        \n",
    "        #Achievement\n",
    "        output5 = self.hidden_layer5(encoded_sents)\n",
    "        output5 = self.relu(output5)\n",
    "        output5 = self.classifier5(output5)\n",
    "        output5 = torch.nn.Sigmoid()(output5)\n",
    "        \n",
    "        middle2 = self.middle_layer2(encoded_sents)\n",
    "        middle2 = self.relu(middle2)\n",
    "        \n",
    "        #Power: dominance\n",
    "        output6 = self.hidden_layer6(middle2)\n",
    "        output6 = self.relu(output6)\n",
    "        output6 = self.classifier6(output6)\n",
    "        output6 = torch.nn.Sigmoid()(output6)\n",
    "        \n",
    "        #Power: resources\n",
    "        output7 = self.hidden_layer7(middle2)\n",
    "        output7 = self.relu(output7)\n",
    "        output7 = self.classifier7(output7)\n",
    "        output7 = torch.nn.Sigmoid()(output7)\n",
    "        \n",
    "        #Face\n",
    "        output8 = self.hidden_layer8(encoded_sents)\n",
    "        output8 = self.relu(output8)\n",
    "        output8 = self.classifier8(output8)\n",
    "        output8 = torch.nn.Sigmoid()(output8)\n",
    "        \n",
    "        middle3 = self.middle_layer3(encoded_sents)\n",
    "        middle3 = self.relu(middle3)\n",
    "        \n",
    "        #Security: personal\n",
    "        output9 = self.hidden_layer9(middle3)\n",
    "        output9 = self.relu(output9)\n",
    "        output9 = self.classifier9(output9)\n",
    "        output9 = torch.nn.Sigmoid()(output9)\n",
    "        \n",
    "        #Security: societal\n",
    "        output10 = self.hidden_layer10(middle3)\n",
    "        output10 = self.relu(output10)\n",
    "        output10 = self.classifier10(output10)\n",
    "        output10 = torch.nn.Sigmoid()(output10)\n",
    "        \n",
    "        #Tradition\n",
    "        output11 = self.hidden_layer11(encoded_sents)\n",
    "        output11 = self.relu(output11)\n",
    "        output11 = self.classifier11(output11)\n",
    "        output11 = torch.nn.Sigmoid()(output11)\n",
    "        \n",
    "        middle4 = self.middle_layer4(encoded_sents)\n",
    "        middle4 = self.relu(middle4)\n",
    "        \n",
    "        #Conformity: rules\n",
    "        output12 = self.hidden_layer12(middle4)\n",
    "        output12 = self.relu(output12)\n",
    "        output12 = self.classifier12(output12)\n",
    "        output12 = torch.nn.Sigmoid()(output12)\n",
    "        \n",
    "        #Conformity: interpersonal\n",
    "        output13 = self.hidden_layer13(middle4)\n",
    "        output13 = self.relu(output13)\n",
    "        output13 = self.classifier13(output13)\n",
    "        output13 = torch.nn.Sigmoid()(output13)\n",
    "        \n",
    "        #Humility\n",
    "        output14 = self.hidden_layer14(encoded_sents)\n",
    "        output14 = self.relu(output14)\n",
    "        output14 = self.classifier14(output14)\n",
    "        output14 = torch.nn.Sigmoid()(output14)\n",
    "        \n",
    "        middle5 = self.middle_layer5(encoded_sents)\n",
    "        middle5 = self.relu(middle5)\n",
    "        \n",
    "        #Benevolence: caring\n",
    "        output15 = self.hidden_layer15(middle5)\n",
    "        output15 = self.relu(output15)\n",
    "        output15 = self.classifier15(output15)\n",
    "        output15 = torch.nn.Sigmoid()(output15)\n",
    "        \n",
    "        #Benevolence: dependability\n",
    "        output16 = self.hidden_layer16(middle5)\n",
    "        output16 = self.relu(output16)\n",
    "        output16 = self.classifier16(output16)\n",
    "        output16 = torch.nn.Sigmoid()(output16)\n",
    "        \n",
    "        middle6 = self.middle_layer6(encoded_sents)\n",
    "        middle6 = self.relu(middle6)\n",
    "        output_middle6 = self.classifier_middle6(middle6)\n",
    "        output_middle6 = torch.nn.Sigmoid()(output_middle6)\n",
    "        \n",
    "        #Universalism: concern\n",
    "        output17 = self.hidden_layer17(middle6)\n",
    "        output17 = self.relu(output17)\n",
    "        output17 = self.classifier17(output17)\n",
    "        output17 = torch.nn.Sigmoid()(output17)\n",
    "        \n",
    "        #Universalism: nature\n",
    "        output18 = self.hidden_layer18(middle6)\n",
    "        output18 = self.relu(output18)\n",
    "        output18 = self.classifier18(output18)\n",
    "        output18 = torch.nn.Sigmoid()(output18)\n",
    "        \n",
    "        #Universalism: tolerance\n",
    "        output19 = self.hidden_layer19(middle6)\n",
    "        output19 = self.relu(output19)\n",
    "        output19 = self.classifier19(output19)\n",
    "        output19 = torch.nn.Sigmoid()(output19)\n",
    "        \n",
    "        #Universalism: objectivity\n",
    "        output20 = self.hidden_layer20(middle6)\n",
    "        output20 = self.relu(output20)\n",
    "        output20 = self.classifier20(output20)\n",
    "        output20 = torch.nn.Sigmoid()(output20)\n",
    "        #output = self.hidden_layer2(output)\n",
    "        #output = self.relu(output)\n",
    "        #output = self.hidden_layer3(output)\n",
    "        #output = self.relu(output)\n",
    "        #output = self.hidden_layer4(output)\n",
    "        #output = self.relu(output)\n",
    "        #output = self.classifier(output)\n",
    "        #return self.log_softmax(output)\n",
    "        return output, output2, output3, output4, output5, output6, output7, output8, output9, output10, output11, output12, output13, output14, output15, output16, output17, output18, output19, output20, output_middle6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "6Bcjvav41o1d"
   },
   "outputs": [],
   "source": [
    "# class NLIClassifier(torch.nn.Module):\n",
    "#     def __init__(self, output_size: int, hidden_size: int):\n",
    "#         super().__init__()\n",
    "#         self.output_size = output_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         # Initialize BERT, which we use instead of a single embedding layer.\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         # TODO [OPTIONAL]: Updating all BERT parameters can be slow and memory intensive. \n",
    "#         # Freeze them if training is too slow. Notice that the learning\n",
    "#         # rate should probably be smaller in this case.\n",
    "#         # Uncommenting out the below 2 lines means only our classification layer will be updated.\n",
    "#         for param in self.bert.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         self.bert_hidden_dimension = self.bert.config.hidden_size\n",
    "#         print(self.bert_hidden_dimension)\n",
    "#         # TODO: Add an extra hidden layer in the classifier, projecting\n",
    "#         #      from the BERT hidden dimension to hidden size.\n",
    "#         # TODO: Add a relu nonlinearity to be used in the forward method\n",
    "#         #      https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "#         self.hidden_layer1 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer2 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer3 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer4 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer5 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer6 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer7 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer8 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer9 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer10 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer11 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer12 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer13 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer14 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer15 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer16 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer17 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer18 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer19 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         self.hidden_layer20 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
    "#         # self.hidden_layers = [torch.nn.Linear(self.bert_hidden_dimension, 32).to(device) for i in range(self.output_size)]\n",
    "#         # self.classifiers = [torch.nn.Linear(32, 1).to(device) for i in range(self.output_size)]\n",
    "#         # #self.hidden_layer2 = torch.nn.Linear(self.hidden_size, 32)\n",
    "#         #self.hidden_layer3 = torch.nn.Linear(128, 32)\n",
    "#         #self.hidden_layer4 = torch.nn.Linear(32, 8)\n",
    "#         self.hidden_layers = []\n",
    "#         for i in range(20):\n",
    "#             self.hidden_layers.append(torch.nn.Linear(self.bert_hidden_dimension, 32))\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "#         self.classifier1 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier2 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier3 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier4 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier5 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier6 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier7 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier8 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier9 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier10 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier11 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier12 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier13 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier14 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier15 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier16 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier17 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier18 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier19 = torch.nn.Linear(32, 1)\n",
    "#         self.classifier20 = torch.nn.Linear(32, 1)\n",
    "#         self.classifiers = []\n",
    "#         for i in range(20):\n",
    "#           self.classifiers.append(torch.nn.Linear(32, 1))\n",
    "#         #self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "#         self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "#     def encode_text(\n",
    "#         self,\n",
    "#         symbols: Dict\n",
    "#     ) -> torch.Tensor:\n",
    "#         \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
    "#             Then, get the last (non-padded) hidden state for each symbol and return that.\n",
    "\n",
    "#         Args:\n",
    "#             symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
    "#                 the entire sentence\n",
    "#         \"\"\"\n",
    "#         # First we get the contextualized embedding for each input symbol\n",
    "#         # We no longer need an LSTM, since BERT encodes context and \n",
    "#         # gives us a single vector describing the sequence in the form of the [CLS] token.\n",
    "#         embedded = self.bert(**symbols)\n",
    "#         #print(embedded)\n",
    "#         #print(\"Embedded\", embedded.pooler_output.shape, embedded.last_hidden_state.shape)\n",
    "#         # TODO: Get the [CLS] token using the `pooler_output` from \n",
    "#         #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "#         #      and check the returns for the forward method.\n",
    "#         # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
    "#         #raise NotImplementedError\n",
    "        \n",
    "#         #pool_output_shape = embedded.pooler_output.shape\n",
    "#         #return torch.reshape(embedded.pooler_output,(pool_output_shape[0],1,pool_output_shape[1]) )\n",
    "#         last_hidden_state = embedded.last_hidden_state[:,0,:]\n",
    "#         hidden_shape = last_hidden_state.shape\n",
    "#         return torch.reshape(last_hidden_state,(hidden_shape[0],1,hidden_shape[1]) )\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         symbols: Dict,\n",
    "#     ) -> torch.Tensor:\n",
    "#         \"\"\"_summary_\n",
    "\n",
    "#         Args:\n",
    "#             symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: _description_\n",
    "#         \"\"\"\n",
    "#         encoded_sents = self.encode_text(symbols)\n",
    "#         #output = self.hidden_layer1(encoded_sents)\n",
    "#         #output = self.relu(output)\n",
    "#         #outputs = [self.hidden_layers[i](encoded_sents) for i in range(self.output_size)]\n",
    "#         #outputs = [self.relu(outputs[i].to(device)) for i in range(self.output_size)]\n",
    "#         #outputs = [self.classifiers[i](outputs[i].to(device)) for i in range(self.output_size)]\n",
    "#         # outputs = []\n",
    "#         # for i in range(self.output_size):\n",
    "#         #     output = self.hidden_layers[i](encoded_sents)\n",
    "#         #     output = self.relu(output)\n",
    "#         #     output = self.classifiers[i](output)\n",
    "#         #     output = torch.nn.Sigmoid()(output)\n",
    "#         #     outputs.append(output)\n",
    "#         # output = self.hidden_layer1(encoded_sents)\n",
    "#         # output = self.relu(output)\n",
    "#         # output = self.classifier1(output)\n",
    "#         # output = torch.nn.Sigmoid()(output)\n",
    "        \n",
    "#         # output2 = self.hidden_layer2(encoded_sents)\n",
    "#         # output2 = self.relu(output2)\n",
    "#         # output2 = self.classifier2(output2)\n",
    "#         # output2 = torch.nn.Sigmoid()(output2)\n",
    "        \n",
    "#         # output3 = self.hidden_layer1(encoded_sents)\n",
    "#         # output3 = self.relu(output3)\n",
    "#         # output3 = self.classifier1(output3)\n",
    "#         # output3 = torch.nn.Sigmoid()(output3)\n",
    "        \n",
    "#         # output4 = self.hidden_layer4(encoded_sents)\n",
    "#         # output4 = self.relu(output4)\n",
    "#         # output4 = self.classifier4(output4)\n",
    "#         # output4 = torch.nn.Sigmoid()(output4)\n",
    "        \n",
    "#         # output5 = self.hidden_layer5(encoded_sents)\n",
    "#         # output5 = self.relu(output5)\n",
    "#         # output5 = self.classifier5(output5)\n",
    "#         # output5 = torch.nn.Sigmoid()(output5)\n",
    "        \n",
    "#         # output6 = self.hidden_layer6(encoded_sents)\n",
    "#         # output6 = self.relu(output6)\n",
    "#         # output6 = self.classifier6(output6)\n",
    "#         # output6 = torch.nn.Sigmoid()(output6)\n",
    "        \n",
    "#         # output7 = self.hidden_layer7(encoded_sents)\n",
    "#         # output7 = self.relu(output7)\n",
    "#         # output7 = self.classifier7(output7)\n",
    "#         # output7 = torch.nn.Sigmoid()(output7)\n",
    "        \n",
    "#         # output8 = self.hidden_layer8(encoded_sents)\n",
    "#         # output8 = self.relu(output8)\n",
    "#         # output8 = self.classifier8(output8)\n",
    "#         # output8 = torch.nn.Sigmoid()(output8)\n",
    "        \n",
    "#         # output9 = self.hidden_layer9(encoded_sents)\n",
    "#         # output9 = self.relu(output9)\n",
    "#         # output9 = self.classifier9(output9)\n",
    "#         # output9 = torch.nn.Sigmoid()(output9)\n",
    "        \n",
    "#         # output10 = self.hidden_layer10(encoded_sents)\n",
    "#         # output10 = self.relu(output10)\n",
    "#         # output10 = self.classifier10(output10)\n",
    "#         # output10 = torch.nn.Sigmoid()(output10)\n",
    "        \n",
    "#         # output11 = self.hidden_layer11(encoded_sents)\n",
    "#         # output11 = self.relu(output11)\n",
    "#         # output11 = self.classifier11(output11)\n",
    "#         # output11 = torch.nn.Sigmoid()(output11)\n",
    "        \n",
    "#         # output12 = self.hidden_layer12(encoded_sents)\n",
    "#         # output12 = self.relu(output12)\n",
    "#         # output12 = self.classifier12(output12)\n",
    "#         # output12 = torch.nn.Sigmoid()(output12)\n",
    "        \n",
    "#         # output13 = self.hidden_layer13(encoded_sents)\n",
    "#         # output13 = self.relu(output13)\n",
    "#         # output13 = self.classifier13(output13)\n",
    "#         # output13 = torch.nn.Sigmoid()(output13)\n",
    "        \n",
    "#         # output14 = self.hidden_layer14(encoded_sents)\n",
    "#         # output14 = self.relu(output14)\n",
    "#         # output14 = self.classifier14(output14)\n",
    "#         # output14 = torch.nn.Sigmoid()(output14)\n",
    "\n",
    "#         # output15 = self.hidden_layer15(encoded_sents)\n",
    "#         # output15 = self.relu(output15)\n",
    "#         # output15 = self.classifier15(output15)\n",
    "#         # output15 = torch.nn.Sigmoid()(output15)\n",
    "        \n",
    "#         # output16 = self.hidden_layer16(encoded_sents)\n",
    "#         # output16 = self.relu(output16)\n",
    "#         # output16 = self.classifier16(output16)\n",
    "#         # output16 = torch.nn.Sigmoid()(output16)\n",
    "\n",
    "#         # output17 = self.hidden_layer17(encoded_sents)\n",
    "#         # output17 = self.relu(output17)\n",
    "#         # output17 = self.classifier17(output17)\n",
    "#         # output17 = torch.nn.Sigmoid()(output17)\n",
    "        \n",
    "#         # output18 = self.hidden_layer18(encoded_sents)\n",
    "#         # output18 = self.relu(output18)\n",
    "#         # output18 = self.classifier18(output18)\n",
    "#         # output18 = torch.nn.Sigmoid()(output18)\n",
    "\n",
    "#         # output19 = self.hidden_layer19(encoded_sents)\n",
    "#         # output19 = self.relu(output19)\n",
    "#         # output19 = self.classifier19(output19)\n",
    "#         # output19 = torch.nn.Sigmoid()(output19)\n",
    "        \n",
    "#         # output20 = self.hidden_layer20(encoded_sents)\n",
    "#         # output20 = self.relu(output20)\n",
    "#         # output20 = self.classifier20(output20)\n",
    "#         # output20 = torch.nn.Sigmoid()(output20)\n",
    "#         # #output = self.hidden_layer2(output)\n",
    "#         # #output = self.relu(output)\n",
    "#         # #output = self.hidden_layer3(output)\n",
    "#         # #output = self.relu(output)\n",
    "#         # #output = self.hidden_layer4(output)\n",
    "#         # #output = self.relu(output)\n",
    "#         # #output = self.classifier(output)\n",
    "#         # #return self.log_softmax(output)\n",
    "#         # return output, output2, output3, output4, output5, output6, output7, output8, output9, output10, output11, output12, output13, output14, output15, output16, output17, output18, output19, output20\n",
    "#         outputs = []\n",
    "#         for i in range(20):\n",
    "#             output = self.hidden_layers[i](encoded_sents)\n",
    "#             output = self.relu(output)\n",
    "#             output = self.classifiers[i](output)\n",
    "#             output = torch.nn.Sigmoid()(output)\n",
    "#             outputs.append(output)\n",
    "#         return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "w56Kynj22y80"
   },
   "outputs": [],
   "source": [
    "# For making predictions at test time TODO: Multi-label\n",
    "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
    "    sents = sents.to(device)\n",
    "    logits = model(sents)\n",
    "    res = []\n",
    "    logitslen = logits[0].shape[0]\n",
    "    for i in range(logitslen):\n",
    "        datares = []\n",
    "        for j in range(20):\n",
    "            datares.append(logits[j][i][0][0] > 0.5)\n",
    "        res.append(datares)\n",
    "    return res\n",
    "    #return list(torch.argmax(logits, axis=2).squeeze().numpy())\n",
    "    #print(torch.max(logits), torch.min(logits))\n",
    "    #return list((logits>0).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "cnSswFRd3WzN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy import logical_and, sum as t_sum\n",
    "def precision(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Precision is True Positives / All Positives Predictions\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(pred_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def recall(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Recall is True Positives / All Positive Labels\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(true_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def f1_score(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    which_label: int\n",
    "):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \"\"\"\n",
    "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
    "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
    "    if P and R:\n",
    "        return 2*P*R/(P+R)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def macro_f1(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    possible_labels: List[int]\n",
    "):\n",
    "    scores = [f1_score(predicted_labels, true_labels, l) for l in possible_labels]\n",
    "    # Macro, so we take the uniform avg.\n",
    "    print(scores)\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "lVZr4mEb81_f"
   },
   "outputs": [],
   "source": [
    "def f1Score_multiLabel(preds, labels):\n",
    "    nLabels = 20\n",
    "    relevants = [0]*20\n",
    "    positives = [0]*20\n",
    "    truePositives = [0]*20\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(nLabels):\n",
    "            if(preds[i][j]==1):\n",
    "                positives[j] += 1\n",
    "                if(labels[i][j]==1):\n",
    "                    truePositives[j] += 1\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(nLabels):\n",
    "            if(labels[i][j]==1):\n",
    "                relevants[j] += 1\n",
    "    \n",
    "    precisions = []*nLabels\n",
    "    recalls = []*nLabels\n",
    "    f1Scores = []*nLabels\n",
    "    precision =0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    #print(truePositives, positives, relevants)\n",
    "    for i in range(nLabels):\n",
    "        if(positives[i]>0):\n",
    "            precision = truePositives[i]/positives[i]\n",
    "        precisions.append(precision)\n",
    "        if(relevants[i]>0):\n",
    "            recall = truePositives[i]/relevants[i]\n",
    "        recalls.append(recall)\n",
    "        #print(precision,recall,i)\n",
    "        if(precision>0 and recall>0):\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1Scores.append(f1)\n",
    "    precision_mean = np.mean(precisions)\n",
    "    recall_mean = np.mean(recalls)\n",
    "    f1_mean = np.mean(f1Scores)\n",
    "    return f1_mean, precision_mean, recall_mean\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "IyE6cJhM3i9M"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "def training_loop(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_sents,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    model,\n",
    "):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.BCELoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        for features, labels in tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            features = features.to(device)\n",
    "            labels = labels.float()\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #preds0, preds1, preds2, preds3, preds4, preds5, preds6, preds7, preds8, preds9, preds10, preds11, preds12, preds13, preds14, preds15, preds16, preds17, preds18, preds19  = model(features)\n",
    "            preds = model(features)\n",
    "\n",
    "            #print(preds[0].shape)\n",
    "            #featlen = preds[0].shape[0]\n",
    "            #preds_temp = torch.empty((featlen, 20), dtype=torch.float)\n",
    "            #for k in range(featlen):\n",
    "            #    for j in range(20):\n",
    "            #        preds_temp[k][j] = preds[j][k][0][0]\n",
    "            #preds = preds.squeeze(1)\n",
    "            #print(\"Preds \",preds.shape)\n",
    "            #print(\"Labels \", labels.shape)\n",
    "            #preds_temp = preds_temp.to(device)\n",
    "            #print(preds_temp.is_cuda, labels.is_cuda)\n",
    "            #print(preds_temp.shape, labels.shape)\n",
    "            #loss = loss_func(preds_temp, labels)\n",
    "            #losses = [0]*20\n",
    "            #for i in range(20):\n",
    "            #    loss1 = loss_func(preds[i],labels[:,i])\n",
    "            #    losses.append(loss1)\n",
    "            #print(preds[0].squeeze(1).squeeze(1).shape, labels[:,0].shape)\n",
    "            loss0 = loss_func(preds[0].squeeze(1).squeeze(1), labels[:,0])\n",
    "            #loss = loss_func(preds0.squeeze(1), labels)\n",
    "            loss1 = loss_func(preds[1].squeeze(1).squeeze(1), labels[:,1]) \n",
    "            loss2 = loss_func(preds[2].squeeze(1).squeeze(1), labels[:,2]) \n",
    "            loss3 = loss_func(preds[3].squeeze(1).squeeze(1), labels[:,3]) \n",
    "            loss4 = loss_func(preds[4].squeeze(1).squeeze(1), labels[:,4]) \n",
    "            loss5 = loss_func(preds[5].squeeze(1).squeeze(1), labels[:,5]) \n",
    "            loss6 = loss_func(preds[6].squeeze(1).squeeze(1), labels[:,6]) \n",
    "            loss7 = loss_func(preds[7].squeeze(1).squeeze(1), labels[:,7]) \n",
    "            loss8 = loss_func(preds[8].squeeze(1).squeeze(1), labels[:,8]) \n",
    "            loss9 = loss_func(preds[9].squeeze(1).squeeze(1), labels[:,9]) \n",
    "            loss10 = loss_func(preds[10].squeeze(1).squeeze(1), labels[:,10])\n",
    "            loss11 = loss_func(preds[11].squeeze(1).squeeze(1), labels[:,11]) \n",
    "            loss12 = loss_func(preds[12].squeeze(1).squeeze(1), labels[:,12]) \n",
    "            loss13 = loss_func(preds[13].squeeze(1).squeeze(1), labels[:,13]) \n",
    "            loss14 = loss_func(preds[14].squeeze(1).squeeze(1), labels[:,14]) \n",
    "            loss15 = loss_func(preds[15].squeeze(1).squeeze(1), labels[:,15]) \n",
    "            loss16 = loss_func(preds[16].squeeze(1).squeeze(1), labels[:,16]) \n",
    "            loss17 = loss_func(preds[17].squeeze(1).squeeze(1), labels[:,17]) \n",
    "            loss18 = loss_func(preds[18].squeeze(1).squeeze(1), labels[:,18]) \n",
    "            loss19 = loss_func(preds[19].squeeze(1).squeeze(1), labels[:,19])\n",
    "            group6_loss = loss_func(preds[20].squeeze(1).squeeze(1), labels[:,20])\n",
    "            #Multiply group loss by number of elements in group and add all groups\n",
    "            global_loss = group6_loss*4.0 \n",
    "            local_loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6 + loss7 + loss8 + loss9 + loss10 + loss11 + loss12 + loss13 + loss14 + loss15 + loss16 + loss17 + loss18 + loss19\n",
    "            loss = 1.0*global_loss + local_loss\n",
    "            # Backpropogate the loss through our model\n",
    "            #loss.register_hook(lambda grad: print(grad))\n",
    "            #print(model.hidden_layers[0].weight.grad)\n",
    "            #print(loss.grad)\n",
    "            loss = loss*1000\n",
    "            #print(i,model.hidden_layers[0].weight)\n",
    "            loss.backward()\n",
    "            #print(model.hidden_layers[0].weight.grad)\n",
    "            #print(loss.grad)\n",
    "            optimizer.step()\n",
    "            #print(\"After\",i, model.hidden_layers[0].weight)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {np.sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating dev...\")\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "            sents = sents.to(device)\n",
    "            pred = predict(model, sents)\n",
    "            all_preds.extend(pred)\n",
    "            all_labels.extend(list(labels))\n",
    "        # #print(range(len(set(train_labels))))\n",
    "\n",
    "        dev_f1, dev_P, dev_R = f1Score_multiLabel(all_preds, all_labels)\n",
    "        print(f\"Dev F1 {dev_f1},  Dev Precision {dev_P}, Dev Recall {dev_R}\")\n",
    "        scheduler.step()\n",
    "        #print(optimizer)\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "N6NQ1Vpb3z2k"
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "# def training_loop(\n",
    "#     num_epochs,\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     dev_sents,\n",
    "#     dev_labels,\n",
    "#     optimizer,\n",
    "#     #scheduler,\n",
    "#     model,\n",
    "# ):\n",
    "#     print(\"Training...\")\n",
    "#     loss_func = torch.nn.BCELoss()\n",
    "#     batches = list(zip(train_features, train_labels))\n",
    "#     random.shuffle(batches)\n",
    "#     for i in range(num_epochs):\n",
    "#         losses = []\n",
    "#         for features, labels in tqdm(batches):\n",
    "#             # Empty the dynamic computation graph\n",
    "#             features = features.to(device)\n",
    "#             labels = labels.float()\n",
    "#             labels = labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             #preds0, preds1, preds2, preds3, preds4, preds5, preds6, preds7, preds8, preds9, preds10, preds11, preds12, preds13, preds14, preds15, preds16, preds17, preds18, preds19  = model(features)\n",
    "#             preds = model(features)\n",
    "\n",
    "#             #print(preds[0].shape)\n",
    "#             #featlen = preds[0].shape[0]\n",
    "#             #preds_temp = torch.empty((featlen, 20), dtype=torch.float)\n",
    "#             #for k in range(featlen):\n",
    "#             #    for j in range(20):\n",
    "#             #        preds_temp[k][j] = preds[j][k][0][0]\n",
    "#             #preds = preds.squeeze(1)\n",
    "#             #print(\"Preds \",preds.shape)\n",
    "#             #print(\"Labels \", labels.shape)\n",
    "#             #preds_temp = preds_temp.to(device)\n",
    "#             #print(preds_temp.is_cuda, labels.is_cuda)\n",
    "#             #print(preds_temp.shape, labels.shape)\n",
    "#             #loss = loss_func(preds_temp, labels)\n",
    "#             #losses = [0]*20\n",
    "#             #for i in range(20):\n",
    "#             #    loss1 = loss_func(preds[i],labels[:,i])\n",
    "#             #    losses.append(loss1)\n",
    "#             #print(preds[0].squeeze(1).squeeze(1).shape, labels[:,0].shape)\n",
    "#             loss0 = loss_func(preds[0].squeeze(1).squeeze(1), labels[:,0])\n",
    "#             #loss = loss_func(preds0.squeeze(1), labels)\n",
    "#             loss1 = loss_func(preds[1].squeeze(1).squeeze(1), labels[:,1]) \n",
    "#             loss2 = loss_func(preds[2].squeeze(1).squeeze(1), labels[:,2]) \n",
    "#             loss3 = loss_func(preds[3].squeeze(1).squeeze(1), labels[:,3]) \n",
    "#             loss4 = loss_func(preds[4].squeeze(1).squeeze(1), labels[:,4]) \n",
    "#             loss5 = loss_func(preds[5].squeeze(1).squeeze(1), labels[:,5]) \n",
    "#             loss6 = loss_func(preds[6].squeeze(1).squeeze(1), labels[:,6]) \n",
    "#             loss7 = loss_func(preds[7].squeeze(1).squeeze(1), labels[:,7]) \n",
    "#             loss8 = loss_func(preds[8].squeeze(1).squeeze(1), labels[:,8]) \n",
    "#             loss9 = loss_func(preds[9].squeeze(1).squeeze(1), labels[:,9]) \n",
    "#             loss10 = loss_func(preds[10].squeeze(1).squeeze(1), labels[:,10])\n",
    "#             loss11 = loss_func(preds[11].squeeze(1).squeeze(1), labels[:,11]) \n",
    "#             loss12 = loss_func(preds[12].squeeze(1).squeeze(1), labels[:,12]) \n",
    "#             loss13 = loss_func(preds[13].squeeze(1).squeeze(1), labels[:,13]) \n",
    "#             loss14 = loss_func(preds[14].squeeze(1).squeeze(1), labels[:,14]) \n",
    "#             loss15 = loss_func(preds[15].squeeze(1).squeeze(1), labels[:,15]) \n",
    "#             loss16 = loss_func(preds[16].squeeze(1).squeeze(1), labels[:,16]) \n",
    "#             loss17 = loss_func(preds[17].squeeze(1).squeeze(1), labels[:,17]) \n",
    "#             loss18 = loss_func(preds[18].squeeze(1).squeeze(1), labels[:,18]) \n",
    "#             loss19 = loss_func(preds[19].squeeze(1).squeeze(1), labels[:,19])  \n",
    "#             loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6 + loss7 + loss8 + loss9 + loss10 + loss11 + loss12 + loss13 + loss14 + loss15 + loss16 + loss17 + loss18 + loss19\n",
    "#             # Backpropogate the loss through our model\n",
    "#             #loss.register_hook(lambda grad: print(grad))\n",
    "#             #print(model.hidden_layers[0].weight.grad)\n",
    "#             #print(loss.grad)\n",
    "#             loss = loss*1000\n",
    "#             #print(i,model.hidden_layers[0].weight)\n",
    "#             loss.backward()\n",
    "#             #print(model.hidden_layers[0].weight.grad)\n",
    "#             #print(loss.grad)\n",
    "#             optimizer.step()\n",
    "#             #print(\"After\",i, model.hidden_layers[0].weight)\n",
    "#             losses.append(loss.item())\n",
    "        \n",
    "#         print(f\"epoch {i}, loss: {np.sum(losses)/len(losses)}\")\n",
    "#         # Estimate the f1 score for the development set\n",
    "#         print(\"Evaluating dev...\")\n",
    "#         all_preds = []\n",
    "#         all_labels = []\n",
    "#         for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "#             sents = sents.to(device)\n",
    "#             pred = predict(model, sents)\n",
    "#             all_preds.extend(pred)\n",
    "#             all_labels.extend(list(labels))\n",
    "#         # #print(range(len(set(train_labels))))\n",
    "\n",
    "#         dev_f1, dev_P, dev_R = f1Score_multiLabel(all_preds, all_labels)\n",
    "#         print(f\"Dev F1 {dev_f1},  Dev Precision {dev_P}, Dev Recall {dev_R}\")\n",
    "#         # # #scheduler.step()\n",
    "#         #print(optimizer)\n",
    "#     # Return the trained model\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liqb8oDn4fNx",
    "outputId": "db4ad508-ab04-47b6-eed2-aea0408fd627",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "epochs = 10\n",
    "epoch_warmup = 5 #Ideally 1/6 i.e 50 for 300 epochs\n",
    "# TODO: Find a good learning rate\n",
    "LR = 1e-4\n",
    "\n",
    "possible_labels = 20\n",
    "model = NLIClassifier(output_size=possible_labels, hidden_size=512)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 20,epochs)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "lwia5LTU7xJ2"
   },
   "outputs": [],
   "source": [
    "# LR = 1e-5\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "f31ed555a96c41d5aaa68caabc76866d",
      "970e32a86c63433ea101472fd43f92b8",
      "0c8d79fd83b6446e87b85aa5ee92bfe5",
      "29f158b67f894d03ac8b7491eb03504e",
      "d06a135d961d4969abaf1cad88e6bf66",
      "0f0873e2b35c4229871cff349515244b",
      "9991d19282cd400a8c1f35cf1a8b4708",
      "df1ca5011c0f49b8a020bcaa5fa869bf",
      "ecc69437241c4b458d261f4d424a6d82",
      "dffb7f8565c345a7b55c7d4f8023650c",
      "ea99c0f55fad447ca7f38e3522f1d640",
      "a7d1e6f194cd49e68de6e68990327f1f",
      "81845f59726947c19d9c2867babd7b42",
      "4aa54fe2353945ba8cd0063f309b7ae5",
      "eb19be658f994fedb355ffd515c67e3d",
      "8215eb94142b4e38b02295e692beacb8",
      "f5486594d52544f18652f1c2ed40e431",
      "d3b004739a104c1a8788a50a1c6820c2",
      "115afbec1bf14fb88d48b741aac2b556",
      "a937ca14107e4473b5598843a3cc70f0",
      "8c24bf5994e048eb9b1cee0440e34660",
      "ade0149ca54f434f993dc5205c0bcf7b"
     ]
    },
    "id": "Y0ThGzo64w64",
    "outputId": "f06fa622-ba30-4698-93a3-1166d4f554ea",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-104-021a6f6d70d9>:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for features, labels in tqdm(batches):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372101f706fc4b50847724ad65650d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 12011.919674479166\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-104-021a6f6d70d9>:88: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb95476269f440768d653ffb8e0d3e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.009803921568627449,  Dev Precision 0.33333333333333337, Dev Recall 0.0004975124378109452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay Narasimha\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12162023b464116808f9d3570dcdfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 9282.385182291666\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43012c2fb2d44d828c1d4456f1fe17c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.11965811965811965,  Dev Precision 0.4242424242424242, Dev Recall 0.006965174129353234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b524dcbbab406bb3bafcf7bfb73508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 8821.266979166667\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7387ca2b27ea43e79f1c9311c3e3e246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.029431818181818187,  Dev Precision 0.48809523809523814, Dev Recall 0.012573209899867748\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1994221afd05440382f299d7470df5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 8655.963209635416\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4048d8a1384fc1a3b8a750a3f66bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.1697761457524186,  Dev Precision 0.41644736842105257, Dev Recall 0.026544492726242208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a2381430cd4d4e96957c3e11b3422e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 8512.527083333332\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5442d82366e64a638cf24ff5a18ed181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.12204436938832668,  Dev Precision 0.4253459119496855, Dev Recall 0.032075614598559496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a093ced8c7774d6b94fd3fef018e83ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss: 8375.685266927083\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebc95c1f5f147d89d4ca6042de7d274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.1920064361991879,  Dev Precision 0.7891829442244992, Dev Recall 0.04147200373880099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c95eafbe81437897926032a7cbbc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss: 8241.64322265625\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a925cbba2684f8cbbe5d45602f23374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.1611828348374378,  Dev Precision 0.6682172557172559, Dev Recall 0.0637017956011701\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38e0cd9c5834a488d583cfcc2d059b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss: 8114.855735677083\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad03ed264b443eda5ad18d864fa860f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.20688082762190727,  Dev Precision 0.6857666877641898, Dev Recall 0.07896024593793663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c524e4636b48b2901427413c1f0ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss: 7996.27130859375\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edaaae9cd4e4b6786efed8593a7e65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.24735997048949945,  Dev Precision 0.604939833092617, Dev Recall 0.09071974651666308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af46edcf8b44f84a3f60786887d87b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss: 7886.575227864583\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a4f1d6a1d3425498d59dabbcfae0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.29002459696447475,  Dev Precision 0.6882396470828096, Dev Recall 0.10180415812714702\n"
     ]
    }
   ],
   "source": [
    "model =training_loop(\n",
    "    epochs,\n",
    "    train_input_batches,\n",
    "    train_label_batches,\n",
    "    val_input_batches,\n",
    "    val_label_batches,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Lclassifiers-grouped.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('Lclassifiers-grouped.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[  101,  2302,  7040,  ...,     0,     0,     0],\n",
       "         [  101,  3510,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  5970,  2064,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2065,  3008,  ...,     0,     0,     0],\n",
       "         [  101,  2296,  2775,  ...,     0,     0,     0],\n",
       "         [  101,  3478, 16012,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2529, 18856,  ...,     0,     0,     0],\n",
       "         [  101, 26572, 22864,  ...,     0,     0,     0],\n",
       "         [  101,  2591,  2865,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2000,  2108,  ...,     0,     0,     0],\n",
       "         [  101,  5907,  8699,  ...,     0,     0,     0],\n",
       "         [  101,  2529,  4424,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2529, 18856,  ...,     0,     0,     0],\n",
       "         [  101,  2994,  2012,  ...,     0,     0,     0],\n",
       "         [  101,  2686,  8993,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 18504,  2015,  ...,     0,     0,     0],\n",
       "         [  101,  2336,  2071,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  2071,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1045,  2228,  ...,     0,     0,     0],\n",
       "         [  101,  9896,  2594,  ...,     0,     0,     0],\n",
       "         [  101,  2049,  1996,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 12706, 10107,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2406,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101, 11513, 14920,  ...,     0,     0,     0],\n",
       "         [  101, 12706,  2008,  ...,     0,     0,     0],\n",
       "         [  101,  2107,  1037,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2450,  2323,  ...,     0,     0,     0],\n",
       "         [  101, 10078,  8145,  ...,     0,     0,     0],\n",
       "         [  101,  7789,  3200,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1999,  1037,  ...,     0,     0,     0],\n",
       "         [  101,  1996, 15823,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2277,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2057,  2342,  ...,     0,     0,     0],\n",
       "         [  101,  2027,  2031,  ...,     0,     0,     0],\n",
       "         [  101,  3584,  2270,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 2057, 2323,  ...,    0,    0,    0],\n",
       "         [ 101, 2023, 7126,  ...,    0,    0,    0],\n",
       "         [ 101, 2591, 2865,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 2651, 1005,  ...,    0,    0,    0],\n",
       "         [ 101, 3435, 2833,  ...,    0,    0,    0],\n",
       "         [ 101, 6469, 2729,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1996, 15823,  ...,     0,     0,     0],\n",
       "         [  101,  2686,  8993,  ...,     0,     0,     0],\n",
       "         [  101,  2012, 26036,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  8268, 16841,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2024,  ...,     0,     0,     0],\n",
       "         [  101,  7789,  3200,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 9846, 4517,  ...,    0,    0,    0],\n",
       "         [ 101, 2065, 4372,  ...,    0,    0,    0],\n",
       "         [ 101, 2057, 2323,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1996, 2277,  ...,    0,    0,    0],\n",
       "         [ 101, 2686, 8993,  ...,    0,    0,    0],\n",
       "         [ 101, 5907, 8699,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101, 15016,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  4942,  5332,  ...,     0,     0,     0],\n",
       "         [  101, 25536,  5970,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  8392,  3765,  ...,     0,     0,     0],\n",
       "         [  101,  2000,  6855,  ...,     0,     0,     0],\n",
       "         [  101,  2012, 26036,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  7789,  3200,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2045,  2003,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2775,  5889,  ...,     0,     0,     0],\n",
       "         [  101,  2043, 15016,  ...,     0,     0,     0],\n",
       "         [  101, 19297,  2964,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2591,  2865,  ...,     0,     0,     0],\n",
       "         [  101,  3423,  6026,  ...,     0,     0,     0],\n",
       "         [  101,  8696,  2147,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  9896,  2594,  ...,     0,     0,     0],\n",
       "         [  101,  3171, 17147,  ...,     0,     0,     0],\n",
       "         [  101, 25536,  5970,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  7872,  3526,  ...,     0,     0,     0],\n",
       "         [  101,  3510,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  2064,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  4942,  5332,  ...,     0,     0,     0],\n",
       "         [  101,  5210,  5255,  ...,     0,     0,     0],\n",
       "         [  101, 28086,  8713,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2082, 11408,  ...,     0,     0,     0],\n",
       "         [  101,  5717,  1011,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2308,  2031,  ...,     0,     0,     0],\n",
       "         [  101,  4454,  5852,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  2052,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101, 13099,  2495,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2009,  2038,  ...,     0,     0,     0],\n",
       "         [  101,  2270, 12534,  ...,     0,     0,     0],\n",
       "         [  101,  2082,  7083,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2111,  2323,  ...,  8145, 22486,   102],\n",
       "         [  101, 27352,  2895,  ...,     0,     0,     0],\n",
       "         [  101,  2065,  3674,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  8696,  2147,  ...,     0,     0,     0],\n",
       "         [  101,  2591,  2865,  ...,     0,     0,     0],\n",
       "         [  101,  4288, 17967,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2450,  2031,  ...,     0,     0,     0],\n",
       "         [  101,  4517,  5195,  ...,     0,     0,     0],\n",
       "         [  101,  7789,  3200,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2116,  2111,  ...,  7125, 14574,   102],\n",
       "         [  101,  8696,  2147,  ...,     0,     0,     0],\n",
       "         [  101,  6794,  2024,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2096,  1996,  ...,     0,     0,     0],\n",
       "         [  101,  8268, 16841,  ...,     0,     0,     0],\n",
       "         [  101,  3647,  7258,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  8083,  1998,  ...,     0,     0,     0],\n",
       "         [  101,  2049,  2025,  ...,     0,     0,     0],\n",
       "         [  101,  2308,  2031,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  4911,  1996,  ...,     0,     0,     0],\n",
       "         [  101,  3674,  4243,  ...,     0,     0,     0],\n",
       "         [  101,  2547, 16605,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  9201,  2015,  ...,     0,     0,     0],\n",
       "         [  101,  7221,  2399,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2308,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2011,  4942,  ...,  3611,  2015,   102],\n",
       "         [  101,  1996,  5210,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1996,  4386,  ...,     0,     0,     0],\n",
       "         [  101, 15016,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  2045,  2003,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2256,  4277,  ...,     0,     0,     0],\n",
       "         [  101,  2547,  2323,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  5255,  2115,  ...,     0,     0,     0],\n",
       "         [  101,  8268, 16841,  ...,     0,     0,     0],\n",
       "         [  101,  2138,  6469,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2111,  2024,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2093,  ...,     0,     0,     0],\n",
       "         [  101,  4713,  7876,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2065, 10607,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  4942,  5332,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2066,  2060,  ...,     0,     0,     0],\n",
       "         [  101, 26572, 22864,  ...,     0,     0,     0],\n",
       "         [  101,  2019, 17151,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  6620, 26635,  ...,     0,     0,     0],\n",
       "         [  101, 23687,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  9896,  2594,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  8145, 22486,  ...,     0,     0,     0],\n",
       "         [  101,  6469,  5014,  ...,     0,     0,     0],\n",
       "         [  101,  9201,  2015,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2009,  7126,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  4386,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2547,  3475,  ...,     0,     0,     0],\n",
       "         [  101,  2062,  7206,  ...,     0,     0,     0],\n",
       "         [  101,  4368,  2515,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1996,  3125,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  6620, 26635,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  7494,  1037,  ...,     0,     0,     0],\n",
       "         [  101,  6274,  1037,  ...,     0,     0,     0],\n",
       "         [  101, 10093, 14545,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  9201,  2015,  ...,     0,     0,     0],\n",
       "         [  101,  2493,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  5762, 11268,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101, 16151,  2019,  ...,     0,     0,     0],\n",
       "         [  101,  7197,  5920,  ...,     0,     0,     0],\n",
       "         [  101,  2296,  3412,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2064,  2599,  ...,     0,     0,     0],\n",
       "         [  101, 23845,  2003,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2619,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  5807,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 16789,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  3007,  7750,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2224,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1996, 10093,  ...,     0,     0,     0],\n",
       "         [  101,  2082, 11408,  ...,     0,     0,     0],\n",
       "         [  101,  8696,  2147,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2062,  4243,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  1012,  ...,     0,     0,     0],\n",
       "         [  101,  7505,  3217,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  3510,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  2116,  2493,  ...,     0,     0,     0],\n",
       "         [  101,  2270, 12534,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2009,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  2110, 21762,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2224,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  3423,  6026,  ...,     0,     0,     0],\n",
       "         [  101, 25536,  5970,  ...,     0,     0,     0],\n",
       "         [  101,  2053,  1010,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2082,  6375,  ...,     0,     0,     0],\n",
       "         [  101, 25536,  5970,  ...,     0,     0,     0],\n",
       "         [  101,  7079, 16948,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2111,  2031,  ...,     0,     0,     0],\n",
       "         [  101,  4454,  5852,  ...,     0,     0,     0],\n",
       "         [  101,  2070,  2111,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2065, 10093,  ...,     0,     0,     0],\n",
       "         [  101,  3071,  2038,  ...,     0,     0,     0],\n",
       "         [  101,  3923,  3989,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 3923, 3989,  ...,    0,    0,    0],\n",
       "         [ 101, 2009, 2003,  ...,    0,    0,    0],\n",
       "         [ 101, 3071, 2038,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 8083, 2064,  ...,    0,    0,    0],\n",
       "         [ 101, 2009, 2003,  ...,    0,    0,    0],\n",
       "         [ 101, 2057, 2323,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2686,  8993,  ...,     0,     0,     0],\n",
       "         [  101,  2478,  2270,  ...,     0,     0,     0],\n",
       "         [  101,  5014,  9905,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1996, 10831,  ...,     0,     0,     0],\n",
       "         [  101,  3510, 13367,  ...,     0,     0,     0],\n",
       "         [  101, 25536,  5970,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 9896, 2594,  ...,    0,    0,    0],\n",
       "         [ 101, 2070, 5305,  ...,    0,    0,    0],\n",
       "         [ 101, 3423, 6026,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 4454, 5852,  ...,    0,    0,    0],\n",
       "         [ 101, 1996, 4386,  ...,    0,    0,    0],\n",
       "         [ 101, 3647, 7258,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  8145, 22486,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  3957,  ...,     0,     0,     0],\n",
       "         [  101,  9896,  2594,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2065,  2057,  ...,     0,     0,     0],\n",
       "         [  101, 10093, 14545,  ...,     0,     0,     0],\n",
       "         [  101, 13951,  2007,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2270, 12534,  ...,     0,     0,     0],\n",
       "         [  101,  7197,  5920,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  4386,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  3171, 17147,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  2529, 18856,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2057,  5807,  ...,     0,     0,     0],\n",
       "         [  101,  2302,  1037,  ...,     0,     0,     0],\n",
       "         [  101,  3348,  4989,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2025,  2438,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  3007, 15404,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2116,  9201,  ...,     0,     0,     0],\n",
       "         [  101,  6794,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2188, 29477,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2019, 17151,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2093,  ...,     0,     0,     0],\n",
       "         [  101,  8696,  2147,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  7197,  5920,  ...,     0,     0,     0],\n",
       "         [  101,  2122, 26635,  ...,     0,     0,     0],\n",
       "         [  101, 26264,  5852,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  7505,  3217,  ...,     0,     0,     0],\n",
       "         [  101,  2205,  2116,  ...,     0,     0,     0],\n",
       "         [  101, 19297,  2964,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  4855, 11595,  ...,     0,     0,     0],\n",
       "         [  101,  2797,  2510,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2231,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1996,  2157,  ...,     0,     0,     0],\n",
       "         [  101,  8392,  3765,  ...,     0,     0,     0],\n",
       "         [  101,  2529, 18856,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 27352,  2895,  ...,     0,     0,     0],\n",
       "         [  101,  3510,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  4386,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  8268, 16841,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  4386,  ...,     0,     0,     0],\n",
       "         [  101, 16948,  2038,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2057,  5807,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2342,  ...,     0,     0,     0],\n",
       "         [  101,  1996, 19076,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 1037, 2450,  ...,    0,    0,    0],\n",
       "         [ 101, 2152, 3237,  ...,    0,    0,    0],\n",
       "         [ 101, 2000, 6509,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1996, 2561,  ...,    0,    0,    0],\n",
       "         [ 101, 2070, 2163,  ...,    0,    0,    0],\n",
       "         [ 101, 2111, 4855,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  8392,  3765,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2270,  ...,     0,     0,     0],\n",
       "         [  101,  1037,  5717,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2195,  2111,  ...,     0,     0,     0],\n",
       "         [  101,  2035, 20859,  ...,     0,     0,     0],\n",
       "         [  101,  2116,  7243,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1996,  3036,  ...,     0,     0,     0],\n",
       "         [  101,  2053,  2028,  ...,     0,     0,     0],\n",
       "         [  101, 14717,  8385,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2205,  2172,  ...,     0,     0,     0],\n",
       "         [  101,  6620, 11498,  ...,     0,     0,     0],\n",
       "         [  101,  2082,  7083,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  8491, 12743,  ...,     0,     0,     0],\n",
       "         [  101,  2591,  6125,  ...,     0,     0,     0],\n",
       "         [  101,  4942,  5332,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2529, 18856,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2188, 29477,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  6064,  2038,  ...,  2204,  2518,   102],\n",
       "         [  101,  3923,  3989,  ...,     0,     0,     0],\n",
       "         [  101, 23094,  3016,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  4942,  5332,  ...,     0,     0,     0],\n",
       "         [  101,  2045,  2024,  ...,     0,     0,     0],\n",
       "         [  101,  1999,  1037,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2009,  5320,  ...,     0,     0,     0],\n",
       "         [  101, 26635,  3288,  ...,     0,     0,     0],\n",
       "         [  101,  2065,  2057,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1045,  5993,  ...,     0,     0,     0],\n",
       "         [  101,  2591,  2865,  ...,     0,     0,     0],\n",
       "         [  101,  2994,  2012,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 2087, 6469,  ...,    0,    0,    0],\n",
       "         [ 101, 6469, 2729,  ...,    0,    0,    0],\n",
       "         [ 101, 2521, 2205,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1996, 4386,  ...,    0,    0,    0],\n",
       "         [ 101, 6469, 2729,  ...,    0,    0,    0],\n",
       "         [ 101, 8392, 3765,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101, 13099,  2495,  ...,     0,     0,     0],\n",
       "         [  101,  2111,  5247,  ...,     0,     0,     0],\n",
       "         [  101,  2065,  1996,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  8145, 22486,  ...,     0,     0,     0],\n",
       "         [  101,  4517,  4255,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  2003,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2450,  2024,  ...,     0,     0,     0],\n",
       "         [  101,  2045,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  4386,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2082, 12583,  ...,     0,     0,     0],\n",
       "         [  101,  6416,  2064,  ...,     0,     0,     0],\n",
       "         [  101,  2151,  6685,  ...,  4735, 15226,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])},\n",
       " {'input_ids': tensor([[  101,  4942,  5332,  ...,     0,     0,     0],\n",
       "         [  101,  2012, 26036,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2277,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2797,  2510,  ...,     0,     0,     0],\n",
       "         [  101,  8392,  3765,  ...,     0,     0,     0],\n",
       "         [  101, 25536,  5970,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  7505,  3217,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  2064,  ...,     0,     0,     0],\n",
       "         [  101, 19297,  2964,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2009,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  2591,  2865,  ...,     0,     0,     0],\n",
       "         [  101,  2082, 11408,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2308,  1998,  ...,     0,     0,     0],\n",
       "         [  101,  4138,  2111,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2342,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  4372,  6494,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  6202,  ..., 14767,  1012,   102],\n",
       "         [  101,  2053,  6772,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  6469,  2729,  ...,     0,     0,     0],\n",
       "         [  101,  2004,  2111,  ...,     0,     0,     0],\n",
       "         [  101,  2111,  2024,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  3923,  3989,  ...,     0,     0,     0],\n",
       "         [  101, 28086,  8713,  ...,     0,     0,     0],\n",
       "         [  101,  2065,  1037,  ...,  9326,  4277,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])},\n",
       " {'input_ids': tensor([[  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2007,  1037,  ...,     0,     0,     0],\n",
       "         [  101,  3171, 17147,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 11513, 14920,  ...,     0,     0,     0],\n",
       "         [  101,  2045,  2003,  ...,  4517,  4255,   102],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2308,  2024,  ...,     0,     0,     0],\n",
       "         [  101,  9896,  2594,  ...,     0,     0,     0],\n",
       "         [  101, 23094,  3016,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  3647,  7258,  ...,     0,     0,     0],\n",
       "         [  101,  2023,  2052,  ...,     0,     0,     0],\n",
       "         [  101,  2547,  2003,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 1996, 2157,  ...,    0,    0,    0],\n",
       "         [ 101, 2065, 2017,  ...,    0,    0,    0],\n",
       "         [ 101, 2027, 2024,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1997, 2035,  ...,    0,    0,    0],\n",
       "         [ 101, 2188, 2495,  ...,    0,    0,    0],\n",
       "         [ 101, 1996, 3316,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101, 10915,  5075,  ...,     0,     0,     0],\n",
       "         [  101, 10093, 14545,  ...,     0,     0,     0],\n",
       "         [  101,  4713,  7876,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  5907,  4989,  ...,     0,     0,     0],\n",
       "         [  101,  1996, 19076,  ...,     0,     0,     0],\n",
       "         [  101, 16948,  2003,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2009,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  4735,  2097,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1996,  6469,  ...,     0,     0,     0],\n",
       "         [  101, 10093, 14545,  ...,     0,     0,     0],\n",
       "         [  101, 14085, 10976,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1996,  2224,  ...,     0,     0,     0],\n",
       "         [  101,  2065,  1996,  ...,     0,     0,     0],\n",
       "         [  101,  9201,  2015,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2188, 29477,  ...,     0,     0,     0],\n",
       "         [  101,  2087, 23687,  ...,     0,     0,     0],\n",
       "         [  101,  2859,  1005,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2308,  4995,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2025,  4352,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 23094,  3016,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  5807,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  6469,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2009,  2003,  ...,     0,     0,     0],\n",
       "         [  101, 28086,  8713,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2277,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 14770,  6830,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  2036,  2170,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2045,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  4942,  5332,  ...,     0,     0,     0],\n",
       "         [  101, 23687, 18454,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 17985,  3423,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  3800,  ...,     0,     0,     0],\n",
       "         [  101,  3171, 17147,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2188, 29477,  ...,     0,     0,     0],\n",
       "         [  101, 13099,  2495,  ...,     0,     0,     0],\n",
       "         [  101,  2381,  2003,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2859,  1005,  ...,     0,     0,     0],\n",
       "         [  101,  3007,  7750,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2228,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1996, 18634,  ...,  5854,  1012,   102],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2916,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  5762, 11268,  ...,     0,     0,     0],\n",
       "         [  101,  4713,  7876,  ...,     0,     0,     0],\n",
       "         [  101,  2591,  2865,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  8392,  3765,  ...,     0,     0,     0],\n",
       "         [  101,  2144,  9896,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2069,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 14085, 10976,  ...,     0,     0,     0],\n",
       "         [  101,  7505, 21799,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  2323,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  8973,  1999,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  4107,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2406,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1037,  2048,  ...,     0,     0,     0],\n",
       "         [  101,  8268, 16841,  ...,     0,     0,     0],\n",
       "         [  101,  2057,  3685,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101, 10915,  5075,  ...,     0,     0,     0],\n",
       "         [  101,  3617,  7258,  ...,     0,     0,     0],\n",
       "         [  101,  2200,  2411,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2057,  5807,  ...,     0,     0,     0],\n",
       "         [  101, 25536,  5970,  ...,     0,     0,     0],\n",
       "         [  101,  9201,  2015,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2009,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  9896,  2594,  ...,     0,     0,     0],\n",
       "         [  101, 13156,  6240,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2116,  2111,  ...,     0,     0,     0],\n",
       "         [  101,  2188, 29477,  ...,     0,     0,     0],\n",
       "         [  101,  2188, 29477,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  9896,  2594,  ...,     0,     0,     0],\n",
       "         [  101, 26572, 22864,  ...,     0,     0,     0],\n",
       "         [  101,  5907,  1011,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  8392,  3765,  ...,     0,     0,     0],\n",
       "         [  101,  4785,  2689,  ...,     0,     0,     0],\n",
       "         [  101,  9416,  4288,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 1037, 3423,  ...,    0,    0,    0],\n",
       "         [ 101, 6950, 2179,  ...,    0,    0,    0],\n",
       "         [ 101, 3751, 3765,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 2027, 5326,  ..., 2022, 7917,  102],\n",
       "         [ 101, 7789, 3200,  ...,    0,    0,    0],\n",
       "         [ 101, 1996, 2783,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1996, 13296,  ..., 14767,  1012,   102],\n",
       "         [  101,  9245,  2024,  ...,     0,     0,     0],\n",
       "         [  101,  1999, 26572,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2045,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  5819,  2009,  ...,     0,     0,     0],\n",
       "         [  101,  8268, 16841,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 9416, 4288,  ...,    0,    0,    0],\n",
       "         [ 101, 3923, 3989,  ...,    0,    0,    0],\n",
       "         [ 101, 4637, 8083,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1996, 3751,  ...,    0,    0,    0],\n",
       "         [ 101, 6997, 2008,  ...,    0,    0,    0],\n",
       "         [ 101, 5210, 5255,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xyarnd1xoP-L",
    "outputId": "2ec625d5-b711-4f83-f8d4-3beb6e7bc754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 21]) torch.Size([64, 59])\n",
      "torch.Size([64, 21]) torch.Size([64, 69])\n",
      "torch.Size([64, 21]) torch.Size([64, 57])\n",
      "torch.Size([64, 21]) torch.Size([64, 71])\n",
      "torch.Size([64, 21]) torch.Size([64, 72])\n",
      "torch.Size([64, 21]) torch.Size([64, 52])\n",
      "torch.Size([64, 21]) torch.Size([64, 101])\n",
      "torch.Size([64, 21]) torch.Size([64, 56])\n",
      "torch.Size([64, 21]) torch.Size([64, 61])\n",
      "torch.Size([64, 21]) torch.Size([64, 84])\n",
      "torch.Size([64, 21]) torch.Size([64, 53])\n",
      "torch.Size([64, 21]) torch.Size([64, 81])\n",
      "torch.Size([64, 21]) torch.Size([64, 111])\n",
      "torch.Size([64, 21]) torch.Size([64, 98])\n",
      "torch.Size([64, 21]) torch.Size([64, 53])\n",
      "torch.Size([64, 21]) torch.Size([64, 50])\n",
      "torch.Size([64, 21]) torch.Size([64, 60])\n",
      "torch.Size([64, 21]) torch.Size([64, 140])\n",
      "torch.Size([64, 21]) torch.Size([64, 56])\n",
      "torch.Size([64, 21]) torch.Size([64, 60])\n",
      "torch.Size([64, 21]) torch.Size([64, 55])\n",
      "torch.Size([64, 21]) torch.Size([64, 58])\n",
      "torch.Size([64, 21]) torch.Size([64, 50])\n",
      "torch.Size([64, 21]) torch.Size([64, 83])\n",
      "torch.Size([64, 21]) torch.Size([64, 67])\n",
      "torch.Size([64, 21]) torch.Size([64, 56])\n",
      "torch.Size([64, 21]) torch.Size([64, 76])\n",
      "torch.Size([64, 21]) torch.Size([64, 83])\n",
      "torch.Size([64, 21]) torch.Size([64, 55])\n",
      "torch.Size([64, 21]) torch.Size([64, 55])\n",
      "torch.Size([64, 21]) torch.Size([64, 54])\n",
      "torch.Size([64, 21]) torch.Size([64, 54])\n",
      "torch.Size([64, 21]) torch.Size([64, 56])\n",
      "torch.Size([64, 21]) torch.Size([64, 59])\n",
      "torch.Size([64, 21]) torch.Size([64, 54])\n",
      "torch.Size([64, 21]) torch.Size([64, 65])\n",
      "torch.Size([64, 21]) torch.Size([64, 71])\n",
      "torch.Size([64, 21]) torch.Size([64, 53])\n",
      "torch.Size([64, 21]) torch.Size([64, 52])\n",
      "torch.Size([64, 21]) torch.Size([64, 64])\n",
      "torch.Size([64, 21]) torch.Size([64, 142])\n",
      "torch.Size([64, 21]) torch.Size([64, 50])\n",
      "torch.Size([64, 21]) torch.Size([64, 54])\n",
      "torch.Size([64, 21]) torch.Size([64, 73])\n",
      "torch.Size([64, 21]) torch.Size([64, 172])\n",
      "torch.Size([64, 21]) torch.Size([64, 59])\n",
      "torch.Size([64, 21]) torch.Size([64, 58])\n",
      "torch.Size([64, 21]) torch.Size([64, 60])\n",
      "torch.Size([64, 21]) torch.Size([64, 54])\n",
      "torch.Size([64, 21]) torch.Size([64, 58])\n",
      "torch.Size([64, 21]) torch.Size([64, 80])\n",
      "torch.Size([64, 21]) torch.Size([64, 59])\n",
      "torch.Size([64, 21]) torch.Size([64, 76])\n",
      "torch.Size([64, 21]) torch.Size([64, 59])\n",
      "torch.Size([64, 21]) torch.Size([64, 59])\n",
      "torch.Size([64, 21]) torch.Size([64, 52])\n",
      "torch.Size([64, 21]) torch.Size([64, 57])\n",
      "torch.Size([64, 21]) torch.Size([64, 60])\n",
      "torch.Size([64, 21]) torch.Size([64, 64])\n",
      "torch.Size([64, 21]) torch.Size([64, 62])\n",
      "torch.Size([64, 21]) torch.Size([64, 60])\n",
      "torch.Size([64, 21]) torch.Size([64, 72])\n",
      "torch.Size([64, 21]) torch.Size([64, 70])\n",
      "torch.Size([64, 21]) torch.Size([64, 107])\n",
      "torch.Size([64, 21]) torch.Size([64, 73])\n",
      "torch.Size([64, 21]) torch.Size([64, 59])\n",
      "torch.Size([64, 21]) torch.Size([64, 83])\n",
      "torch.Size([64, 21]) torch.Size([64, 58])\n",
      "torch.Size([64, 21]) torch.Size([64, 54])\n",
      "torch.Size([64, 21]) torch.Size([64, 59])\n",
      "torch.Size([64, 21]) torch.Size([64, 62])\n",
      "torch.Size([64, 21]) torch.Size([64, 53])\n",
      "torch.Size([64, 21]) torch.Size([64, 121])\n",
      "torch.Size([64, 21]) torch.Size([64, 51])\n",
      "torch.Size([64, 21]) torch.Size([34, 73])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-abd30288b8ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_input_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_label_batches)):\n",
    "    print(train_label_batches[i].shape, train_input_batches[i]['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520,
     "referenced_widgets": [
      "ee66bd0ea5e44abe8547960d2b953dfd",
      "6794b7344ca74ea79e27d62cc2b731e7",
      "46e9ef759c5a4076af228ca215de5f8a",
      "cab1a59fad4042279965587e565d316a",
      "be051014d67743fbb9d84f7079ef0cd2",
      "7e9aaae0e19745f2ac45d9d45bdf21c0",
      "ab3d4f183f6443e88b4a4b78e5bbe1e2",
      "ad16b7f28a1a45e689ec8389b1713bd4",
      "d775d856f78f429fb3235667a49d23e2",
      "3d49b2fe098646e6b8dd092a1e254e4f",
      "9ba695ce3a904eb2a3e602c5cf5e538a"
     ]
    },
    "id": "_4203j4811Mw",
    "outputId": "2e954a67-aa63-41c2-8880-0f849bb20b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-124-ee7ac79b3e67>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for sents, labels in tqdm(zip(val_input_batches, val_label_batches), total=len(val_input_batches)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee66bd0ea5e44abe8547960d2b953dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140, 114, 2, 0, 34, 0, 5, 34, 31, 37, 185, 0, 91, 0, 13, 0, 0, 0, 10, 50] [500, 500, 12, 9, 153, 0, 500, 500, 500, 500, 500, 0, 394, 0, 493, 0, 1, 0, 82, 482] [140, 114, 53, 6, 108, 22, 5, 34, 31, 37, 185, 145, 105, 62, 13, 39, 171, 26, 99, 52]\n",
      "0.28 1.0 0\n",
      "0.228 1.0 1\n",
      "0.16666666666666666 0.03773584905660377 2\n",
      "0.0 0.0 3\n",
      "0.2222222222222222 0.3148148148148148 4\n",
      "0.2222222222222222 0.0 5\n",
      "0.01 1.0 6\n",
      "0.068 1.0 7\n",
      "0.062 1.0 8\n",
      "0.074 1.0 9\n",
      "0.37 1.0 10\n",
      "0.37 0.0 11\n",
      "0.23096446700507614 0.8666666666666667 12\n",
      "0.23096446700507614 0.0 13\n",
      "0.02636916835699797 1.0 14\n",
      "0.02636916835699797 0.0 15\n",
      "0.0 0.0 16\n",
      "0.0 0.0 17\n",
      "0.12195121951219512 0.10101010101010101 18\n",
      "0.1037344398340249 0.9615384615384616 19\n",
      "Dev F1 0.5140882946543324,  Dev Precision 0.140673202059074, Dev Recall 0.5140882946543324\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating dev...\")\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for sents, labels in tqdm(zip(val_input_batches, val_label_batches), total=len(val_input_batches)):\n",
    "    pred = predict(model, sents)\n",
    "    all_preds.extend(pred)\n",
    "    all_labels.extend(list(labels.numpy()))\n",
    "# #print(range(len(set(train_labels))))\n",
    "\n",
    "dev_f1, dev_P, dev_R = f1Score_multiLabel(all_preds, all_labels)\n",
    "print(f\"Dev F1 {dev_f1},  Dev Precision {dev_P}, Dev Recall {dev_R}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNtkjU5-49vL",
    "outputId": "48a3cf8c-a04f-4a12-8343-2362f1c02844"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeRlExU2w8Lp"
   },
   "outputs": [],
   "source": [
    "def f1Score_multiLabel(preds, labels):\n",
    "    nLabels = 20\n",
    "    relevants = [0]*20\n",
    "    positives = [0]*20\n",
    "    truePositives = [0]*20\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(nLabels):\n",
    "            if(preds[i][j]==1):\n",
    "                positives[j] += 1\n",
    "                if(labels[i][j]==1):\n",
    "                    truePositives[j] += 1\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(nLabels):\n",
    "            if(labels[i][j]==1):\n",
    "                relevants[j] += 1\n",
    "    \n",
    "    precisions = []*nLabels\n",
    "    recalls = []*nLabels\n",
    "    f1Scores = []*nLabels\n",
    "    precision =0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    print(truePositives, positives, relevants)\n",
    "    for i in range(nLabels):\n",
    "        if(positives[i]>0):\n",
    "            precision = truePositives[i]/positives[i]\n",
    "        precisions.append(precision)\n",
    "        if(relevants[i]>0):\n",
    "            recall = truePositives[i]/relevants[i]\n",
    "        recalls.append(recall)\n",
    "        print(precision,recall,i)\n",
    "        if(positives[i]>0 and relevants[i]>0):\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1Scores.append(f1)\n",
    "    precision_mean = np.mean(precisions)\n",
    "    recall_mean = np.mean(recalls)\n",
    "    f1_mean = np.mean(recalls)\n",
    "    return f1_mean, precision_mean, recall_mean\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxPeUYO6xlip",
    "outputId": "796fcdcf-7394-4e6c-9b2c-8c0950de84b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_batches[0][0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0B57iw_741yk",
    "outputId": "1e371b64-30dd-48dc-c172-2d3b05ae22a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0589,  0.2156, -1.1975, -0.6472, -0.6455], requires_grad=True) tensor([0.2369, 0.0586, 0.4756, 0.1383, 0.0906])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(5, requires_grad=True)\n",
    "#target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "target = torch.randn(5).softmax(dim=0)\n",
    "print(input, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXnrg3Ph5QRW",
    "outputId": "e99baf0f-73ea-4174-e109-046a835b9387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9810, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRrI3eptcFJd",
    "outputId": "a949b623-97ad-43b1-892f-e8be417181c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1021,  0.0259,     nan, -0.8798,  0.3323]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.log(input)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nWA-g2dDch-j",
    "outputId": "f6767fcf-20c9-4d59-96f3-bdd7e0ba8d61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0537,  0.0045,     nan, -0.0182,  0.0056]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMUDSzOFctdG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwDuG-5pWVVs",
    "outputId": "142e8a75-a001-4973-978f-0e8444b5bd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5930, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.empty(1, dtype=torch.long)\n",
    "target[0] = 1\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4XKS621WmjX",
    "outputId": "6326ce58-1ee0-4e31-f68f-2c3793ab7e34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 10.0\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "losses=[10]\n",
    "print(f\"epoch {i}, loss: {np.sum(losses)/len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIzpG6eFW6pb",
    "outputId": "399968fe-b790-4094-c9ac-efe21a044665"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDPOLhR_XAdC",
    "outputId": "7f909351-0564-4553-bfe5-c1711d6d09b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4920, -0.2966, -0.1552,  0.1589,  0.5150],\n",
       "        [-0.3642,  0.2618,  0.9885,  0.9222, -0.0693]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(2, 5, requires_grad=True)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3H-Loez5VcQ5",
    "outputId": "0a3dfb98-85bd-4c43-d582-ab6c10fa7971"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1552,  0.9885], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = input[:,2]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8NS42NbVqGD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c8d79fd83b6446e87b85aa5ee92bfe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df1ca5011c0f49b8a020bcaa5fa869bf",
      "max": 75,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ecc69437241c4b458d261f4d424a6d82",
      "value": 75
     }
    },
    "0f0873e2b35c4229871cff349515244b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "115afbec1bf14fb88d48b741aac2b556": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29f158b67f894d03ac8b7491eb03504e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dffb7f8565c345a7b55c7d4f8023650c",
      "placeholder": "​",
      "style": "IPY_MODEL_ea99c0f55fad447ca7f38e3522f1d640",
      "value": " 75/75 [00:18&lt;00:00,  3.93it/s]"
     }
    },
    "3d49b2fe098646e6b8dd092a1e254e4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46e9ef759c5a4076af228ca215de5f8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad16b7f28a1a45e689ec8389b1713bd4",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d775d856f78f429fb3235667a49d23e2",
      "value": 8
     }
    },
    "4aa54fe2353945ba8cd0063f309b7ae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_115afbec1bf14fb88d48b741aac2b556",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a937ca14107e4473b5598843a3cc70f0",
      "value": 8
     }
    },
    "6794b7344ca74ea79e27d62cc2b731e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e9aaae0e19745f2ac45d9d45bdf21c0",
      "placeholder": "​",
      "style": "IPY_MODEL_ab3d4f183f6443e88b4a4b78e5bbe1e2",
      "value": "100%"
     }
    },
    "7e9aaae0e19745f2ac45d9d45bdf21c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81845f59726947c19d9c2867babd7b42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5486594d52544f18652f1c2ed40e431",
      "placeholder": "​",
      "style": "IPY_MODEL_d3b004739a104c1a8788a50a1c6820c2",
      "value": "100%"
     }
    },
    "8215eb94142b4e38b02295e692beacb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c24bf5994e048eb9b1cee0440e34660": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "970e32a86c63433ea101472fd43f92b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f0873e2b35c4229871cff349515244b",
      "placeholder": "​",
      "style": "IPY_MODEL_9991d19282cd400a8c1f35cf1a8b4708",
      "value": "100%"
     }
    },
    "9991d19282cd400a8c1f35cf1a8b4708": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ba695ce3a904eb2a3e602c5cf5e538a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7d1e6f194cd49e68de6e68990327f1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81845f59726947c19d9c2867babd7b42",
       "IPY_MODEL_4aa54fe2353945ba8cd0063f309b7ae5",
       "IPY_MODEL_eb19be658f994fedb355ffd515c67e3d"
      ],
      "layout": "IPY_MODEL_8215eb94142b4e38b02295e692beacb8"
     }
    },
    "a937ca14107e4473b5598843a3cc70f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ab3d4f183f6443e88b4a4b78e5bbe1e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad16b7f28a1a45e689ec8389b1713bd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ade0149ca54f434f993dc5205c0bcf7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be051014d67743fbb9d84f7079ef0cd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cab1a59fad4042279965587e565d316a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d49b2fe098646e6b8dd092a1e254e4f",
      "placeholder": "​",
      "style": "IPY_MODEL_9ba695ce3a904eb2a3e602c5cf5e538a",
      "value": " 8/8 [02:22&lt;00:00, 22.09s/it]"
     }
    },
    "d06a135d961d4969abaf1cad88e6bf66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3b004739a104c1a8788a50a1c6820c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d775d856f78f429fb3235667a49d23e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df1ca5011c0f49b8a020bcaa5fa869bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dffb7f8565c345a7b55c7d4f8023650c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea99c0f55fad447ca7f38e3522f1d640": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb19be658f994fedb355ffd515c67e3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c24bf5994e048eb9b1cee0440e34660",
      "placeholder": "​",
      "style": "IPY_MODEL_ade0149ca54f434f993dc5205c0bcf7b",
      "value": " 8/8 [00:01&lt;00:00,  4.94it/s]"
     }
    },
    "ecc69437241c4b458d261f4d424a6d82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ee66bd0ea5e44abe8547960d2b953dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6794b7344ca74ea79e27d62cc2b731e7",
       "IPY_MODEL_46e9ef759c5a4076af228ca215de5f8a",
       "IPY_MODEL_cab1a59fad4042279965587e565d316a"
      ],
      "layout": "IPY_MODEL_be051014d67743fbb9d84f7079ef0cd2"
     }
    },
    "f31ed555a96c41d5aaa68caabc76866d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_970e32a86c63433ea101472fd43f92b8",
       "IPY_MODEL_0c8d79fd83b6446e87b85aa5ee92bfe5",
       "IPY_MODEL_29f158b67f894d03ac8b7491eb03504e"
      ],
      "layout": "IPY_MODEL_d06a135d961d4969abaf1cad88e6bf66"
     }
    },
    "f5486594d52544f18652f1c2ed40e431": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
