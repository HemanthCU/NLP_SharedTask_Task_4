{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIJGTIRqwShE",
        "outputId": "3f3e80b7-e24f-488a-d435-f006a65a3eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 55.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 83.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7SvCi2JuhC5P"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def write_tsv_dataframe(filepath, dataframe):\n",
        "    \"\"\"\n",
        "        Stores `DataFrame` as tsv file\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath : str\n",
        "            Path to tsv file\n",
        "        dataframe : pd.DataFrame\n",
        "            DataFrame to store\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        IOError\n",
        "            if the file can't be opened\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dataframe.to_csv(filepath, encoding='utf-8', sep='\\t', index=False, header=True, quoting=csv.QUOTE_NONE)\n",
        "    except IOError:\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ve5zjHvhhIMT"
      },
      "outputs": [],
      "source": [
        "def combine_columns(df_arguments, df_labels):\n",
        "    \"\"\"Combines the two `DataFrames` on column `Argument ID`\"\"\"\n",
        "    return pd.merge(df_arguments, df_labels, on='Argument ID')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tnaaQHrnhYnh"
      },
      "outputs": [],
      "source": [
        "def split_arguments(df_arguments):\n",
        "    \"\"\"Splits `DataFrame` by column `Usage` into `train`-, `validation`-, and `test`-arguments\"\"\"\n",
        "    train_arguments = df_arguments.loc[df_arguments['Usage'] == 'train'].drop(['Usage'], axis=1).reset_index(drop=True)\n",
        "    valid_arguments = df_arguments.loc[df_arguments['Usage'] == 'validation'].drop(['Usage'], axis=1).reset_index(drop=True)\n",
        "    test_arguments = df_arguments.loc[df_arguments['Usage'] == 'test'].drop(['Usage'], axis=1).reset_index(drop=True)\n",
        "    \n",
        "    return train_arguments, valid_arguments, test_arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kpljNrsChbf1"
      },
      "outputs": [],
      "source": [
        "def create_dataframe_head(argument_ids, model_name):\n",
        "    \"\"\"\n",
        "        Creates `DataFrame` usable to append predictions to it\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        argument_ids : list[str]\n",
        "            First column of the resulting DataFrame\n",
        "        model_name : str\n",
        "            Second column of DataFrame will contain the given model name\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            prepared DataFrame\n",
        "    \"\"\"\n",
        "    df_model_head = pd.DataFrame(argument_ids, columns=['Argument ID'])\n",
        "    df_model_head['Method'] = [model_name] * len(argument_ids)\n",
        "\n",
        "    return df_model_head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1MgX3B55hd9P"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "class MissingColumnError(AttributeError):\n",
        "    \"\"\"Error indicating that an imported DataFrame lacks necessary columns\"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qe-7L7j3ho_X"
      },
      "outputs": [],
      "source": [
        "def load_json_file(filepath):\n",
        "    \"\"\"Load content of json-file from `filepath`\"\"\"\n",
        "    with open(filepath, 'r') as  json_file:\n",
        "        return json.load(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vWGzjs9lhq_s"
      },
      "outputs": [],
      "source": [
        "def load_values_from_json(filepath):\n",
        "    \"\"\"Load values per level from json-file from `filepath`\"\"\"\n",
        "    json_values = load_json_file(filepath)\n",
        "    values = { \"1\":set(), \"2\":set(), \"3\":set(), \"4a\":set(), \"4b\":set() }\n",
        "    for value in json_values[\"values\"]:\n",
        "        values[\"1\"].add(value[\"name\"])\n",
        "        values[\"2\"].add(value[\"level2\"])\n",
        "        for valueLevel3 in value[\"level3\"]:\n",
        "            values[\"3\"].add(valueLevel3)\n",
        "        for valueLevel4a in value[\"level4a\"]:\n",
        "            values[\"4a\"].add(valueLevel4a)\n",
        "        for valueLevel4b in value[\"level4b\"]:\n",
        "            values[\"4b\"].add(valueLevel4b)\n",
        "    values[\"1\"] = sorted(values[\"1\"])\n",
        "    values[\"2\"] = sorted(values[\"2\"])\n",
        "    values[\"3\"] = sorted(values[\"3\"])\n",
        "    values[\"4a\"] = sorted(values[\"4a\"])\n",
        "    values[\"4b\"] = sorted(values[\"4b\"])\n",
        "    return values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HpBKjzIihtk0"
      },
      "outputs": [],
      "source": [
        "def load_arguments_from_tsv(filepath, default_usage='test'):\n",
        "    \"\"\"\n",
        "        Reads arguments from tsv file\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath : str\n",
        "            The path to the tsv file\n",
        "        default_usage : str, optional\n",
        "            The default value if the column \"Usage\" is missing\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            the DataFrame with all arguments\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        MissingColumnError\n",
        "            if the required columns \"Argument ID\" or \"Premise\" are missing in the read data\n",
        "        IOError\n",
        "            if the file can't be read\n",
        "        \"\"\"\n",
        "    try:\n",
        "        dataframe = pd.read_csv(filepath, encoding='utf-8', sep='\\t', header=0)\n",
        "        if not {'Argument ID', 'Premise'}.issubset(set(dataframe.columns.values)):\n",
        "            raise MissingColumnError('The argument \"%s\" file does not contain the minimum required columns [Argument ID, Premise].' % filepath)\n",
        "        if 'Usage' not in dataframe.columns.values:\n",
        "            dataframe['Usage'] = [default_usage] * len(dataframe)\n",
        "        return dataframe\n",
        "    except IOError:\n",
        "        traceback.print_exc()\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lTFyn21VhwsY"
      },
      "outputs": [],
      "source": [
        "def load_labels_from_tsv(filepath, label_order):\n",
        "    \"\"\"\n",
        "        Reads label annotations from tsv file\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath : str\n",
        "            The path to the tsv file\n",
        "        label_order : list[str]\n",
        "            The listing and order of the labels to use from the read data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            the DataFrame with the annotations\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        MissingColumnError\n",
        "            if the required columns \"Argument ID\" or names from `label_order` are missing in the read data\n",
        "        IOError\n",
        "            if the file can't be read\n",
        "        \"\"\"\n",
        "    try:\n",
        "        dataframe = pd.read_csv(filepath, encoding='utf-8', sep='\\t', header=0)\n",
        "        dataframe = dataframe[['Argument ID'] + label_order]\n",
        "        return dataframe\n",
        "    except IOError:\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "    except KeyError:\n",
        "        raise MissingColumnError('The file \"%s\" does not contain the required columns for its level.' % filepath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "yxSNyT8Why1g"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import getopt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "dQ_Eh4qMi6dA"
      },
      "outputs": [],
      "source": [
        "model_dir = 'models'\n",
        "data_dir = 'data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "HHejuM1kh4i8"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "eyHY5YMAi5D9"
      },
      "outputs": [],
      "source": [
        "argument_filepath = os.path.join(data_dir, 'arguments.tsv')\n",
        "value_json_filepath = os.path.join(data_dir, 'values.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "cNk-VDCUjPh1"
      },
      "outputs": [],
      "source": [
        "df_arguments = load_arguments_from_tsv(argument_filepath, default_usage='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "cN1Ugt6xjUla"
      },
      "outputs": [],
      "source": [
        "values = load_values_from_json(value_json_filepath)\n",
        "num_labels_Lv2 = len(values['2'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sZzSzdQmYi7",
        "outputId": "feca34dc-04db-4aac-8359-4aec5ed73c20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Argument ID', 'Part', 'Usage', 'Conclusion', 'Stance', 'Premise'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ],
      "source": [
        "df_arguments.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "gRHUX8b_n9ve"
      },
      "outputs": [],
      "source": [
        "# for ip in df_arguments['Argument ID']:\n",
        "#   #print(df_arguments['Stance'][ip])\n",
        "#   print(ip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "b-L7fgEjnXGK"
      },
      "outputs": [],
      "source": [
        "level =2\n",
        "label_filepath = os.path.join(data_dir, 'labels-level{}.tsv'.format(str(level)))\n",
        "df_labels = load_labels_from_tsv(label_filepath, values[str(level)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIBMeZIfntMM",
        "outputId": "df5ebd58-aadb-4463-8e58-f30d40929552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5270 Argument ID\n",
            "5270 Achievement\n",
            "5270 Benevolence: caring\n",
            "5270 Benevolence: dependability\n",
            "5270 Conformity: interpersonal\n",
            "5270 Conformity: rules\n",
            "5270 Face\n",
            "5270 Hedonism\n",
            "5270 Humility\n",
            "5270 Power: dominance\n",
            "5270 Power: resources\n",
            "5270 Security: personal\n",
            "5270 Security: societal\n",
            "5270 Self-direction: action\n",
            "5270 Self-direction: thought\n",
            "5270 Stimulation\n",
            "5270 Tradition\n",
            "5270 Universalism: concern\n",
            "5270 Universalism: nature\n",
            "5270 Universalism: objectivity\n",
            "5270 Universalism: tolerance\n"
          ]
        }
      ],
      "source": [
        "a = df_labels.keys()\n",
        "for key in df_labels.keys():\n",
        "  print(len(df_labels[key]),key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7ONfVdGtsQ5",
        "outputId": "51cb5577-e2eb-443f-c32b-61a20640873e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "df_labels['Achievement'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "MurNX18XjWIw"
      },
      "outputs": [],
      "source": [
        "# from typing import Dict, List\n",
        "# #def generate_pairwise_input(dataset: Dict[List], labels: Dict[List]) -> (List[str], List[str], List[str], List[int]):\n",
        "# def generate_pairwise_input(dataset, labels):\n",
        "#     \"\"\"\n",
        "#     TODO: group all premises and corresponding hypotheses and labels of the datapoints\n",
        "#     a datapoint as seen earlier is a dict of premis, hypothesis and label\n",
        "#     \"\"\"\n",
        "#     #raise NotImplementedError\n",
        "#     premise=[]\n",
        "#     conclusion=[]\n",
        "#     stance=[]\n",
        "#     n_labels =labels.keys()\n",
        "#     n_labels = n_labels[1:]\n",
        "#     print(n_labels)\n",
        "#     label=[]\n",
        "    \n",
        "#     n = len(dataset['Argument ID'])\n",
        "#     m = len(labels['Argument ID'])\n",
        "#     print(n,m)\n",
        "#     for i in range(n):\n",
        "#         premise.append(dataset['Premise'][i])\n",
        "#         conclusion.append(dataset['Conclusion'][i])\n",
        "#         stance.append(dataset['Stance'][i])\n",
        "#     for l in range(len(n_labels)):\n",
        "#         label_id = []\n",
        "#         #print(i)\n",
        "#         for i in range(m):\n",
        "#             #print(n_labels[l])\n",
        "#             label_id.append(int(labels[n_labels[l]][i]))\n",
        "#         label.append(label_id)\n",
        "\n",
        "#     return premise, conclusion, stance, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "#def generate_pairwise_input(dataset: Dict[List], labels: Dict[List]) -> (List[str], List[str], List[str], List[int]):\n",
        "def generate_pairwise_input(dataset, labels):\n",
        "    \"\"\"\n",
        "    TODO: group all premises and corresponding hypotheses and labels of the datapoints\n",
        "    a datapoint as seen earlier is a dict of premis, hypothesis and label\n",
        "    \"\"\"\n",
        "    #raise NotImplementedError\n",
        "    premise=[]\n",
        "    conclusion=[]\n",
        "    stance=[]\n",
        "    n_labels =labels.keys()\n",
        "    n_labels = n_labels[1:]\n",
        "    print(n_labels)\n",
        "    label=[]\n",
        "    \n",
        "    n = len(dataset['Argument ID'])\n",
        "    m = len(labels['Argument ID'])\n",
        "    print(n,m)\n",
        "    for i in range(n):\n",
        "        premise.append(dataset['Premise'][i])\n",
        "        conclusion.append(dataset['Conclusion'][i])\n",
        "        stance.append(dataset['Stance'][i])\n",
        "    for i in range(m):\n",
        "        sent_label = []\n",
        "        #print(i)\n",
        "        for l in range(len(n_labels)):\n",
        "            #print(n_labels[l])\n",
        "            sent_label.append(int(labels[n_labels[l]][i]))\n",
        "        label.append(sent_label)\n",
        "\n",
        "    return premise, conclusion, stance, label"
      ],
      "metadata": {
        "id": "QoizFWHAVzgp"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBU9bpZwjvH9",
        "outputId": "f9345802-0af1-4d90-9b03-765e58fb52df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Achievement', 'Benevolence: caring', 'Benevolence: dependability',\n",
            "       'Conformity: interpersonal', 'Conformity: rules', 'Face', 'Hedonism',\n",
            "       'Humility', 'Power: dominance', 'Power: resources',\n",
            "       'Security: personal', 'Security: societal', 'Self-direction: action',\n",
            "       'Self-direction: thought', 'Stimulation', 'Tradition',\n",
            "       'Universalism: concern', 'Universalism: nature',\n",
            "       'Universalism: objectivity', 'Universalism: tolerance'],\n",
            "      dtype='object')\n",
            "5270 5270\n"
          ]
        }
      ],
      "source": [
        "#Randomize them first\n",
        "train_premises, train_conclusion, train_stance, train_labels = generate_pairwise_input(df_arguments, df_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "nKP166RxYrGw"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "random.seed(42)\n",
        "def randomize_data(premises, conclusion, stance, labels):\n",
        "  n = len(premises)\n",
        "  data = list(range(n))\n",
        "  random.shuffle(data)\n",
        "  train_premises = []\n",
        "  train_conclusion = []\n",
        "  train_stance = []\n",
        "  train_labels = []\n",
        "  for i in data:\n",
        "    train_premises.append(premises[i])\n",
        "    train_conclusion.append(conclusion[i])\n",
        "    train_stance.append(stance[i])\n",
        "    train_labels.append(labels[:][i])\n",
        "  return train_premises, train_conclusion, train_stance, train_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a = train_labels[:][-5:]\n",
        "# len(a)"
      ],
      "metadata": {
        "id": "N60nWP8mUib_"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "0Ye8wftgaawi"
      },
      "outputs": [],
      "source": [
        "train_premises, train_conclusion, train_stance, train_labels = randomize_data(train_premises, train_conclusion, train_stance, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "o3ygsKZZzBhs"
      },
      "outputs": [],
      "source": [
        "val_premises = train_premises[-500:]\n",
        "val_conclusion = train_conclusion[-500:]\n",
        "val_stance = train_stance[-500:]\n",
        "val_labels = train_labels[:][-500:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "KveGMjWxzpFF"
      },
      "outputs": [],
      "source": [
        "train_premises = train_premises[:-500]\n",
        "train_conclusion = train_conclusion[:-500]\n",
        "train_stance = train_stance[:-500]\n",
        "train_labels = train_labels[:][:-500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhW38nWHv2tj",
        "outputId": "c7c149ac-8c52-4a68-f2bb-d282d16e9d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1996, 18458,  1012,   102,  1999,  7927,  1997,\n",
            "          2023,  2003,  1996, 10744,   102],\n",
            "        [  101,  2023,  2003,  2036,  1037, 18458,   102,  2114,  2023,  2003,\n",
            "          1037,  2117, 10744,   102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] this is the premise. [SEP] in favour of this is the hypothesis [SEP]',\n",
              " '[CLS] this is also a premise [SEP] against this is a second hypothesis [SEP] [PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ],
      "source": [
        "# Nothing to do for this class!\n",
        "import torch\n",
        "from transformers import BertModel\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Dict, List\n",
        "\n",
        "class BatchTokenizer:\n",
        "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes the tokenizer\n",
        "\n",
        "        Args:\n",
        "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\n",
        "        \"\"\"\n",
        "        self.hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    \n",
        "    def get_sep_token(self,):\n",
        "        return self.hf_tokenizer.sep_token\n",
        "    \n",
        "    def __call__(self, prem_batch: List[str], hyp_batch: List[str], stance_batch: List[str]) -> List[List[str]]:\n",
        "        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\n",
        "\n",
        "        We return a dictionary of tensors per the huggingface model specification.\n",
        "\n",
        "        Args:\n",
        "            batch (List[str]): A List of sentence strings\n",
        "\n",
        "        Returns:\n",
        "            Dict: The dictionary of token specifications provided by HuggingFace\n",
        "        \"\"\"\n",
        "        # The HF tokenizer will PAD for us, and additionally combine \n",
        "        # The two sentences deimited by the [SEP] token.\n",
        "        batch_len = len(prem_batch)\n",
        "        #spaces = [\" \"]*batch_len\n",
        "        conc_batch = [stance_batch[i]+\" \"+hyp_batch[i] for i in range(batch_len)]\n",
        "        enc = self.hf_tokenizer(\n",
        "            prem_batch,\n",
        "            conc_batch,\n",
        "            padding=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return enc\n",
        "    \n",
        "\n",
        "# HERE IS AN EXAMPLE OF HOW TO USE THE BATCH TOKENIZER\n",
        "tokenizer = BatchTokenizer()\n",
        "a = [[\"this is the premise.\", \"This is also a premise\"], [\"this is the hypothesis\", \"This is a second hypothesis\"],[\"in favour of\", \"against\"]]\n",
        "x = tokenizer(*a)\n",
        "print(x)\n",
        "tokenizer.hf_tokenizer.batch_decode(x[\"input_ids\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "FQWwRKXgrYRY"
      },
      "outputs": [],
      "source": [
        "def chunk(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[:][i:i + n]\n",
        "\n",
        "def chunk_multi(lst1, lst2, lst3, n):\n",
        "    for i in range(0, len(lst1), n):\n",
        "        yield lst1[i: i + n], lst2[i: i + n], lst3[i: i + n]\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCdddSnCwwmA",
        "outputId": "41520686-e453-40ce-aaba-e8129f6161c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16277\n"
          ]
        }
      ],
      "source": [
        "sum=0\n",
        "import numpy as np\n",
        "# for i in range(5270):\n",
        "#   sum += np.sum(np.array(train_labels[:][i]))\n",
        "print(np.sum(np.array(train_labels)))\n",
        "#print(sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "nchPyT3yAtW3"
      },
      "outputs": [],
      "source": [
        "against=0\n",
        "infavour = 0\n",
        "for i in range(4770):\n",
        "  if(train_stance[i]=='against'):\n",
        "    against +=1\n",
        "  elif(train_stance[i]=='in favor of'):\n",
        "    infavour += 1\n",
        "  else:\n",
        "    print(train_stance[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "v-COBdmXv1Gf"
      },
      "outputs": [],
      "source": [
        "# Notice that since we use huggingface, we tokenize and\n",
        "# encode in all at once!\n",
        "batch_size=64\n",
        "tokenizer = BatchTokenizer()\n",
        "train_input_batches = [b for b in chunk_multi(train_premises, train_conclusion, train_stance, batch_size)]\n",
        "# Tokenize + encode\n",
        "train_input_batches = [tokenizer(*batch) for batch in train_input_batches]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "E-H-JWr6-snT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "W_ta_vSAx4oz"
      },
      "outputs": [],
      "source": [
        "val_input_batches = [b for b in chunk_multi(val_premises, val_conclusion, val_stance, batch_size)]\n",
        "# Tokenize + encode\n",
        "val_input_batches = [tokenizer(*batch) for batch in val_input_batches]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VwgQMkk1ZxW",
        "outputId": "18f0ff1c-71c0-4016-e480-b9252bb0abfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ],
      "source": [
        "len(val_labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "XgY4RO1-05Tz"
      },
      "outputs": [],
      "source": [
        "def encode_labels(labels: List[List[int]]) -> List[torch.FloatTensor]:\n",
        "    \"\"\"Turns the batch of labels into a tensor\n",
        "\n",
        "    Args:\n",
        "        labels (List[List[int]]): List of all labels in the batch\n",
        "\n",
        "    Returns:\n",
        "        List[torch.FloatTensor]: List of Tensors of all labels in the batch\n",
        "    \"\"\"\n",
        "    \n",
        "    return torch.LongTensor(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M67nEmKFR1ar",
        "outputId": "bd9c6a23-8cc5-44d0-c9c8-77732714b00c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "cw7wSXAk19YY"
      },
      "outputs": [],
      "source": [
        "train_label_batches = [b for b in chunk(train_labels, batch_size)]\n",
        "train_label_batches = [encode_labels(batch) for batch in train_label_batches]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "s2Jl56Mg2HdF"
      },
      "outputs": [],
      "source": [
        "val_label_batches = [b for b in chunk(val_labels, batch_size)]\n",
        "val_label_batches = [encode_labels(batch) for batch in val_label_batches]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9i5nn1qT244",
        "outputId": "b00cb0b8-f00e-470f-bbaa-66d44330286d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ],
      "source": [
        "val_label_batches[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "qsgi7io72ewY"
      },
      "outputs": [],
      "source": [
        "class NLIClassifier(torch.nn.Module):\n",
        "    def __init__(self, output_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        # Initialize BERT, which we use instead of a single embedding layer.\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        # TODO [OPTIONAL]: Updating all BERT parameters can be slow and memory intensive. \n",
        "        # Freeze them if training is too slow. Notice that the learning\n",
        "        # rate should probably be smaller in this case.\n",
        "        # Uncommenting out the below 2 lines means only our classification layer will be updated.\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.bert_hidden_dimension = self.bert.config.hidden_size\n",
        "        print(self.bert_hidden_dimension)\n",
        "        # TODO: Add an extra hidden layer in the classifier, projecting\n",
        "        #      from the BERT hidden dimension to hidden size.\n",
        "        # TODO: Add a relu nonlinearity to be used in the forward method\n",
        "        #      https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
        "        self.hidden_layer1 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer2 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer3 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer4 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer5 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer6 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer7 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer8 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer9 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer10 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer11 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer12 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer13 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer14 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer15 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer16 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer17 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer18 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer19 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layer20 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "        self.hidden_layers = [torch.nn.Linear(self.bert_hidden_dimension, 32).to(device) for i in range(self.output_size)]\n",
        "        self.classifiers = [torch.nn.Linear(32, 1).to(device) for i in range(self.output_size)]\n",
        "        #self.hidden_layer2 = torch.nn.Linear(self.hidden_size, 32)\n",
        "        #self.hidden_layer3 = torch.nn.Linear(128, 32)\n",
        "        #self.hidden_layer4 = torch.nn.Linear(32, 8)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.classifier1 = torch.nn.Linear(32, 1)\n",
        "        self.classifier2 = torch.nn.Linear(32, 1)\n",
        "        self.classifier3 = torch.nn.Linear(32, 1)\n",
        "        self.classifier4 = torch.nn.Linear(32, 1)\n",
        "        self.classifier5 = torch.nn.Linear(32, 1)\n",
        "        self.classifier6 = torch.nn.Linear(32, 1)\n",
        "        self.classifier7 = torch.nn.Linear(32, 1)\n",
        "        self.classifier8 = torch.nn.Linear(32, 1)\n",
        "        self.classifier9 = torch.nn.Linear(32, 1)\n",
        "        self.classifier10 = torch.nn.Linear(32, 1)\n",
        "        self.classifier11 = torch.nn.Linear(32, 1)\n",
        "        self.classifier12 = torch.nn.Linear(32, 1)\n",
        "        self.classifier13 = torch.nn.Linear(32, 1)\n",
        "        self.classifier14 = torch.nn.Linear(32, 1)\n",
        "        self.classifier15 = torch.nn.Linear(32, 1)\n",
        "        self.classifier16 = torch.nn.Linear(32, 1)\n",
        "        self.classifier17 = torch.nn.Linear(32, 1)\n",
        "        self.classifier18 = torch.nn.Linear(32, 1)\n",
        "        self.classifier19 = torch.nn.Linear(32, 1)\n",
        "        self.classifier20 = torch.nn.Linear(32, 1)\n",
        "        #self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def encode_text(\n",
        "        self,\n",
        "        symbols: Dict\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
        "            Then, get the last (non-padded) hidden state for each symbol and return that.\n",
        "\n",
        "        Args:\n",
        "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
        "                the entire sentence\n",
        "        \"\"\"\n",
        "        # First we get the contextualized embedding for each input symbol\n",
        "        # We no longer need an LSTM, since BERT encodes context and \n",
        "        # gives us a single vector describing the sequence in the form of the [CLS] token.\n",
        "        embedded = self.bert(**symbols)\n",
        "        #print(embedded)\n",
        "        #print(\"Embedded\", embedded.pooler_output.shape, embedded.last_hidden_state.shape)\n",
        "        # TODO: Get the [CLS] token using the `pooler_output` from \n",
        "        #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
        "        #      and check the returns for the forward method.\n",
        "        # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
        "        #raise NotImplementedError\n",
        "        \n",
        "        #pool_output_shape = embedded.pooler_output.shape\n",
        "        #return torch.reshape(embedded.pooler_output,(pool_output_shape[0],1,pool_output_shape[1]) )\n",
        "        last_hidden_state = embedded.last_hidden_state[:,0,:]\n",
        "        hidden_shape = last_hidden_state.shape\n",
        "        return torch.reshape(last_hidden_state,(hidden_shape[0],1,hidden_shape[1]) )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        symbols: Dict,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: _description_\n",
        "        \"\"\"\n",
        "        encoded_sents = self.encode_text(symbols)\n",
        "        #output = self.hidden_layer1(encoded_sents)\n",
        "        #output = self.relu(output)\n",
        "        #outputs = [self.hidden_layers[i](encoded_sents) for i in range(self.output_size)]\n",
        "        #outputs = [self.relu(outputs[i].to(device)) for i in range(self.output_size)]\n",
        "        #outputs = [self.classifiers[i](outputs[i].to(device)) for i in range(self.output_size)]\n",
        "        # outputs = []\n",
        "        # for i in range(self.output_size):\n",
        "        #     output = self.hidden_layers[i](encoded_sents)\n",
        "        #     output = self.relu(output)\n",
        "        #     output = self.classifiers[i](output)\n",
        "        #     output = torch.nn.Sigmoid()(output)\n",
        "        #     outputs.append(output)\n",
        "        output = self.hidden_layer1(encoded_sents)\n",
        "        output = self.relu(output)\n",
        "        output = self.classifier1(output)\n",
        "        output = torch.nn.Sigmoid()(output)\n",
        "        \n",
        "        output2 = self.hidden_layer2(encoded_sents)\n",
        "        output2 = self.relu(output2)\n",
        "        output2 = self.classifier2(output2)\n",
        "        output2 = torch.nn.Sigmoid()(output2)\n",
        "        \n",
        "        output3 = self.hidden_layer1(encoded_sents)\n",
        "        output3 = self.relu(output3)\n",
        "        output3 = self.classifier1(output3)\n",
        "        output3 = torch.nn.Sigmoid()(output3)\n",
        "        \n",
        "        output4 = self.hidden_layer4(encoded_sents)\n",
        "        output4 = self.relu(output4)\n",
        "        output4 = self.classifier4(output4)\n",
        "        output4 = torch.nn.Sigmoid()(output4)\n",
        "        \n",
        "        output5 = self.hidden_layer5(encoded_sents)\n",
        "        output5 = self.relu(output5)\n",
        "        output5 = self.classifier5(output5)\n",
        "        output5 = torch.nn.Sigmoid()(output5)\n",
        "        \n",
        "        output6 = self.hidden_layer6(encoded_sents)\n",
        "        output6 = self.relu(output6)\n",
        "        output6 = self.classifier6(output6)\n",
        "        output6 = torch.nn.Sigmoid()(output6)\n",
        "        \n",
        "        output7 = self.hidden_layer7(encoded_sents)\n",
        "        output7 = self.relu(output7)\n",
        "        output7 = self.classifier7(output7)\n",
        "        output7 = torch.nn.Sigmoid()(output7)\n",
        "        \n",
        "        output8 = self.hidden_layer8(encoded_sents)\n",
        "        output8 = self.relu(output8)\n",
        "        output8 = self.classifier8(output8)\n",
        "        output8 = torch.nn.Sigmoid()(output8)\n",
        "        \n",
        "        output9 = self.hidden_layer9(encoded_sents)\n",
        "        output9 = self.relu(output9)\n",
        "        output9 = self.classifier9(output9)\n",
        "        output9 = torch.nn.Sigmoid()(output9)\n",
        "        \n",
        "        output10 = self.hidden_layer10(encoded_sents)\n",
        "        output10 = self.relu(output10)\n",
        "        output10 = self.classifier10(output10)\n",
        "        output10 = torch.nn.Sigmoid()(output10)\n",
        "        \n",
        "        output11 = self.hidden_layer11(encoded_sents)\n",
        "        output11 = self.relu(output11)\n",
        "        output11 = self.classifier11(output11)\n",
        "        output11 = torch.nn.Sigmoid()(output11)\n",
        "        \n",
        "        output12 = self.hidden_layer12(encoded_sents)\n",
        "        output12 = self.relu(output12)\n",
        "        output12 = self.classifier12(output12)\n",
        "        output12 = torch.nn.Sigmoid()(output12)\n",
        "        \n",
        "        output13 = self.hidden_layer13(encoded_sents)\n",
        "        output13 = self.relu(output13)\n",
        "        output13 = self.classifier13(output13)\n",
        "        output13 = torch.nn.Sigmoid()(output13)\n",
        "        \n",
        "        output14 = self.hidden_layer14(encoded_sents)\n",
        "        output14 = self.relu(output14)\n",
        "        output14 = self.classifier14(output14)\n",
        "        output14 = torch.nn.Sigmoid()(output14)\n",
        "\n",
        "        output15 = self.hidden_layer15(encoded_sents)\n",
        "        output15 = self.relu(output15)\n",
        "        output15 = self.classifier15(output15)\n",
        "        output15 = torch.nn.Sigmoid()(output15)\n",
        "        \n",
        "        output16 = self.hidden_layer16(encoded_sents)\n",
        "        output16 = self.relu(output16)\n",
        "        output16 = self.classifier16(output16)\n",
        "        output16 = torch.nn.Sigmoid()(output16)\n",
        "\n",
        "        output17 = self.hidden_layer17(encoded_sents)\n",
        "        output17 = self.relu(output17)\n",
        "        output17 = self.classifier17(output17)\n",
        "        output17 = torch.nn.Sigmoid()(output17)\n",
        "        \n",
        "        output18 = self.hidden_layer18(encoded_sents)\n",
        "        output18 = self.relu(output18)\n",
        "        output18 = self.classifier18(output18)\n",
        "        output18 = torch.nn.Sigmoid()(output18)\n",
        "\n",
        "        output19 = self.hidden_layer19(encoded_sents)\n",
        "        output19 = self.relu(output19)\n",
        "        output19 = self.classifier19(output19)\n",
        "        output19 = torch.nn.Sigmoid()(output19)\n",
        "        \n",
        "        output20 = self.hidden_layer20(encoded_sents)\n",
        "        output20 = self.relu(output20)\n",
        "        output20 = self.classifier20(output20)\n",
        "        output20 = torch.nn.Sigmoid()(output20)\n",
        "        #output = self.hidden_layer2(output)\n",
        "        #output = self.relu(output)\n",
        "        #output = self.hidden_layer3(output)\n",
        "        #output = self.relu(output)\n",
        "        #output = self.hidden_layer4(output)\n",
        "        #output = self.relu(output)\n",
        "        #output = self.classifier(output)\n",
        "        #return self.log_softmax(output)\n",
        "        return output, output2, output3, output4, output5, output6, output7, output8, output9, output10, output11, output12, output13, output14, output15, output16, output17, output18, output19, output20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class NLIClassifier(torch.nn.Module):\n",
        "#     def __init__(self, output_size: int, hidden_size: int):\n",
        "#         super().__init__()\n",
        "#         self.output_size = output_size\n",
        "#         self.hidden_size = hidden_size\n",
        "#         # Initialize BERT, which we use instead of a single embedding layer.\n",
        "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "#         # TODO [OPTIONAL]: Updating all BERT parameters can be slow and memory intensive. \n",
        "#         # Freeze them if training is too slow. Notice that the learning\n",
        "#         # rate should probably be smaller in this case.\n",
        "#         # Uncommenting out the below 2 lines means only our classification layer will be updated.\n",
        "#         for param in self.bert.parameters():\n",
        "#             param.requires_grad = False\n",
        "#         self.bert_hidden_dimension = self.bert.config.hidden_size\n",
        "#         print(self.bert_hidden_dimension)\n",
        "#         # TODO: Add an extra hidden layer in the classifier, projecting\n",
        "#         #      from the BERT hidden dimension to hidden size.\n",
        "#         # TODO: Add a relu nonlinearity to be used in the forward method\n",
        "#         #      https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
        "#         self.hidden_layer1 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer2 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer3 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer4 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer5 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer6 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer7 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer8 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer9 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer10 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer11 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer12 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer13 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer14 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer15 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer16 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer17 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer18 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer19 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         self.hidden_layer20 = torch.nn.Linear(self.bert_hidden_dimension, 32)\n",
        "#         # self.hidden_layers = [torch.nn.Linear(self.bert_hidden_dimension, 32).to(device) for i in range(self.output_size)]\n",
        "#         # self.classifiers = [torch.nn.Linear(32, 1).to(device) for i in range(self.output_size)]\n",
        "#         # #self.hidden_layer2 = torch.nn.Linear(self.hidden_size, 32)\n",
        "#         #self.hidden_layer3 = torch.nn.Linear(128, 32)\n",
        "#         #self.hidden_layer4 = torch.nn.Linear(32, 8)\n",
        "#         self.hidden_layers = []\n",
        "#         for i in range(20):\n",
        "#             self.hidden_layers.append(torch.nn.Linear(self.bert_hidden_dimension, 32))\n",
        "#         self.relu = torch.nn.ReLU()\n",
        "#         self.classifier1 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier2 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier3 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier4 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier5 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier6 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier7 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier8 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier9 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier10 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier11 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier12 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier13 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier14 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier15 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier16 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier17 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier18 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier19 = torch.nn.Linear(32, 1)\n",
        "#         self.classifier20 = torch.nn.Linear(32, 1)\n",
        "#         self.classifiers = []\n",
        "#         for i in range(20):\n",
        "#           self.classifiers.append(torch.nn.Linear(32, 1))\n",
        "#         #self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "#         self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "#     def encode_text(\n",
        "#         self,\n",
        "#         symbols: Dict\n",
        "#     ) -> torch.Tensor:\n",
        "#         \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
        "#             Then, get the last (non-padded) hidden state for each symbol and return that.\n",
        "\n",
        "#         Args:\n",
        "#             symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "#         Returns:\n",
        "#             torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
        "#                 the entire sentence\n",
        "#         \"\"\"\n",
        "#         # First we get the contextualized embedding for each input symbol\n",
        "#         # We no longer need an LSTM, since BERT encodes context and \n",
        "#         # gives us a single vector describing the sequence in the form of the [CLS] token.\n",
        "#         embedded = self.bert(**symbols)\n",
        "#         #print(embedded)\n",
        "#         #print(\"Embedded\", embedded.pooler_output.shape, embedded.last_hidden_state.shape)\n",
        "#         # TODO: Get the [CLS] token using the `pooler_output` from \n",
        "#         #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
        "#         #      and check the returns for the forward method.\n",
        "#         # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
        "#         #raise NotImplementedError\n",
        "        \n",
        "#         #pool_output_shape = embedded.pooler_output.shape\n",
        "#         #return torch.reshape(embedded.pooler_output,(pool_output_shape[0],1,pool_output_shape[1]) )\n",
        "#         last_hidden_state = embedded.last_hidden_state[:,0,:]\n",
        "#         hidden_shape = last_hidden_state.shape\n",
        "#         return torch.reshape(last_hidden_state,(hidden_shape[0],1,hidden_shape[1]) )\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         symbols: Dict,\n",
        "#     ) -> torch.Tensor:\n",
        "#         \"\"\"_summary_\n",
        "\n",
        "#         Args:\n",
        "#             symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
        "\n",
        "#         Returns:\n",
        "#             torch.Tensor: _description_\n",
        "#         \"\"\"\n",
        "#         encoded_sents = self.encode_text(symbols)\n",
        "#         #output = self.hidden_layer1(encoded_sents)\n",
        "#         #output = self.relu(output)\n",
        "#         #outputs = [self.hidden_layers[i](encoded_sents) for i in range(self.output_size)]\n",
        "#         #outputs = [self.relu(outputs[i].to(device)) for i in range(self.output_size)]\n",
        "#         #outputs = [self.classifiers[i](outputs[i].to(device)) for i in range(self.output_size)]\n",
        "#         # outputs = []\n",
        "#         # for i in range(self.output_size):\n",
        "#         #     output = self.hidden_layers[i](encoded_sents)\n",
        "#         #     output = self.relu(output)\n",
        "#         #     output = self.classifiers[i](output)\n",
        "#         #     output = torch.nn.Sigmoid()(output)\n",
        "#         #     outputs.append(output)\n",
        "#         # output = self.hidden_layer1(encoded_sents)\n",
        "#         # output = self.relu(output)\n",
        "#         # output = self.classifier1(output)\n",
        "#         # output = torch.nn.Sigmoid()(output)\n",
        "        \n",
        "#         # output2 = self.hidden_layer2(encoded_sents)\n",
        "#         # output2 = self.relu(output2)\n",
        "#         # output2 = self.classifier2(output2)\n",
        "#         # output2 = torch.nn.Sigmoid()(output2)\n",
        "        \n",
        "#         # output3 = self.hidden_layer1(encoded_sents)\n",
        "#         # output3 = self.relu(output3)\n",
        "#         # output3 = self.classifier1(output3)\n",
        "#         # output3 = torch.nn.Sigmoid()(output3)\n",
        "        \n",
        "#         # output4 = self.hidden_layer4(encoded_sents)\n",
        "#         # output4 = self.relu(output4)\n",
        "#         # output4 = self.classifier4(output4)\n",
        "#         # output4 = torch.nn.Sigmoid()(output4)\n",
        "        \n",
        "#         # output5 = self.hidden_layer5(encoded_sents)\n",
        "#         # output5 = self.relu(output5)\n",
        "#         # output5 = self.classifier5(output5)\n",
        "#         # output5 = torch.nn.Sigmoid()(output5)\n",
        "        \n",
        "#         # output6 = self.hidden_layer6(encoded_sents)\n",
        "#         # output6 = self.relu(output6)\n",
        "#         # output6 = self.classifier6(output6)\n",
        "#         # output6 = torch.nn.Sigmoid()(output6)\n",
        "        \n",
        "#         # output7 = self.hidden_layer7(encoded_sents)\n",
        "#         # output7 = self.relu(output7)\n",
        "#         # output7 = self.classifier7(output7)\n",
        "#         # output7 = torch.nn.Sigmoid()(output7)\n",
        "        \n",
        "#         # output8 = self.hidden_layer8(encoded_sents)\n",
        "#         # output8 = self.relu(output8)\n",
        "#         # output8 = self.classifier8(output8)\n",
        "#         # output8 = torch.nn.Sigmoid()(output8)\n",
        "        \n",
        "#         # output9 = self.hidden_layer9(encoded_sents)\n",
        "#         # output9 = self.relu(output9)\n",
        "#         # output9 = self.classifier9(output9)\n",
        "#         # output9 = torch.nn.Sigmoid()(output9)\n",
        "        \n",
        "#         # output10 = self.hidden_layer10(encoded_sents)\n",
        "#         # output10 = self.relu(output10)\n",
        "#         # output10 = self.classifier10(output10)\n",
        "#         # output10 = torch.nn.Sigmoid()(output10)\n",
        "        \n",
        "#         # output11 = self.hidden_layer11(encoded_sents)\n",
        "#         # output11 = self.relu(output11)\n",
        "#         # output11 = self.classifier11(output11)\n",
        "#         # output11 = torch.nn.Sigmoid()(output11)\n",
        "        \n",
        "#         # output12 = self.hidden_layer12(encoded_sents)\n",
        "#         # output12 = self.relu(output12)\n",
        "#         # output12 = self.classifier12(output12)\n",
        "#         # output12 = torch.nn.Sigmoid()(output12)\n",
        "        \n",
        "#         # output13 = self.hidden_layer13(encoded_sents)\n",
        "#         # output13 = self.relu(output13)\n",
        "#         # output13 = self.classifier13(output13)\n",
        "#         # output13 = torch.nn.Sigmoid()(output13)\n",
        "        \n",
        "#         # output14 = self.hidden_layer14(encoded_sents)\n",
        "#         # output14 = self.relu(output14)\n",
        "#         # output14 = self.classifier14(output14)\n",
        "#         # output14 = torch.nn.Sigmoid()(output14)\n",
        "\n",
        "#         # output15 = self.hidden_layer15(encoded_sents)\n",
        "#         # output15 = self.relu(output15)\n",
        "#         # output15 = self.classifier15(output15)\n",
        "#         # output15 = torch.nn.Sigmoid()(output15)\n",
        "        \n",
        "#         # output16 = self.hidden_layer16(encoded_sents)\n",
        "#         # output16 = self.relu(output16)\n",
        "#         # output16 = self.classifier16(output16)\n",
        "#         # output16 = torch.nn.Sigmoid()(output16)\n",
        "\n",
        "#         # output17 = self.hidden_layer17(encoded_sents)\n",
        "#         # output17 = self.relu(output17)\n",
        "#         # output17 = self.classifier17(output17)\n",
        "#         # output17 = torch.nn.Sigmoid()(output17)\n",
        "        \n",
        "#         # output18 = self.hidden_layer18(encoded_sents)\n",
        "#         # output18 = self.relu(output18)\n",
        "#         # output18 = self.classifier18(output18)\n",
        "#         # output18 = torch.nn.Sigmoid()(output18)\n",
        "\n",
        "#         # output19 = self.hidden_layer19(encoded_sents)\n",
        "#         # output19 = self.relu(output19)\n",
        "#         # output19 = self.classifier19(output19)\n",
        "#         # output19 = torch.nn.Sigmoid()(output19)\n",
        "        \n",
        "#         # output20 = self.hidden_layer20(encoded_sents)\n",
        "#         # output20 = self.relu(output20)\n",
        "#         # output20 = self.classifier20(output20)\n",
        "#         # output20 = torch.nn.Sigmoid()(output20)\n",
        "#         # #output = self.hidden_layer2(output)\n",
        "#         # #output = self.relu(output)\n",
        "#         # #output = self.hidden_layer3(output)\n",
        "#         # #output = self.relu(output)\n",
        "#         # #output = self.hidden_layer4(output)\n",
        "#         # #output = self.relu(output)\n",
        "#         # #output = self.classifier(output)\n",
        "#         # #return self.log_softmax(output)\n",
        "#         # return output, output2, output3, output4, output5, output6, output7, output8, output9, output10, output11, output12, output13, output14, output15, output16, output17, output18, output19, output20\n",
        "#         outputs = []\n",
        "#         for i in range(20):\n",
        "#             output = self.hidden_layers[i](encoded_sents)\n",
        "#             output = self.relu(output)\n",
        "#             output = self.classifiers[i](output)\n",
        "#             output = torch.nn.Sigmoid()(output)\n",
        "#             outputs.append(output)\n",
        "#         return outputs\n"
      ],
      "metadata": {
        "id": "6Bcjvav41o1d"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "w56Kynj22y80"
      },
      "outputs": [],
      "source": [
        "# For making predictions at test time TODO: Multi-label\n",
        "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
        "    sents = sents.to(device)\n",
        "    logits = model(sents)\n",
        "    res = []\n",
        "    logitslen = logits[0].shape[0]\n",
        "    for i in range(logitslen):\n",
        "        datares = []\n",
        "        for j in range(20):\n",
        "            datares.append(logits[j][i][0][0] > 0.5)\n",
        "        res.append(datares)\n",
        "    return res\n",
        "    #return list(torch.argmax(logits, axis=2).squeeze().numpy())\n",
        "    #print(torch.max(logits), torch.min(logits))\n",
        "    #return list((logits>0).squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "cnSswFRd3WzN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from numpy import logical_and, sum as t_sum\n",
        "def precision(predicted_labels, true_labels, which_label=1):\n",
        "    \"\"\"\n",
        "    Precision is True Positives / All Positives Predictions\n",
        "    \"\"\"\n",
        "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
        "    true_which = np.array([lab == which_label for lab in true_labels])\n",
        "    denominator = t_sum(pred_which)\n",
        "    if denominator:\n",
        "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def recall(predicted_labels, true_labels, which_label=1):\n",
        "    \"\"\"\n",
        "    Recall is True Positives / All Positive Labels\n",
        "    \"\"\"\n",
        "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
        "    true_which = np.array([lab == which_label for lab in true_labels])\n",
        "    denominator = t_sum(true_which)\n",
        "    if denominator:\n",
        "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def f1_score(\n",
        "    predicted_labels: List[int],\n",
        "    true_labels: List[int],\n",
        "    which_label: int\n",
        "):\n",
        "    \"\"\"\n",
        "    F1 score is the harmonic mean of precision and recall\n",
        "    \"\"\"\n",
        "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
        "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
        "    if P and R:\n",
        "        return 2*P*R/(P+R)\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def macro_f1(\n",
        "    predicted_labels: List[int],\n",
        "    true_labels: List[int],\n",
        "    possible_labels: List[int]\n",
        "):\n",
        "    scores = [f1_score(predicted_labels, true_labels, l) for l in possible_labels]\n",
        "    # Macro, so we take the uniform avg.\n",
        "    print(scores)\n",
        "    return sum(scores) / len(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "lVZr4mEb81_f"
      },
      "outputs": [],
      "source": [
        "def f1Score_multiLabel(preds, labels):\n",
        "    nLabels = 20\n",
        "    relevants = [0]*20\n",
        "    positives = [0]*20\n",
        "    truePositives = [0]*20\n",
        "    for i in range(len(preds)):\n",
        "        for j in range(nLabels):\n",
        "            if(preds[i][j]==1):\n",
        "                positives[j] += 1\n",
        "                if(labels[i][j]==1):\n",
        "                    truePositives[j] += 1\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "        for j in range(nLabels):\n",
        "            if(labels[i][j]==1):\n",
        "                relevants[j] += 1\n",
        "    \n",
        "    precisions = []*nLabels\n",
        "    recalls = []*nLabels\n",
        "    f1Scores = []*nLabels\n",
        "    precision =0\n",
        "    recall = 0\n",
        "    f1 = 0\n",
        "    #print(truePositives, positives, relevants)\n",
        "    for i in range(nLabels):\n",
        "        if(positives[i]>0):\n",
        "            precision = truePositives[i]/positives[i]\n",
        "        precisions.append(precision)\n",
        "        if(relevants[i]>0):\n",
        "            recall = truePositives[i]/relevants[i]\n",
        "        recalls.append(recall)\n",
        "        #print(precision,recall,i)\n",
        "        if(precision>0 and recall>0):\n",
        "            f1 = 2 * precision * recall / (precision + recall)\n",
        "        f1Scores.append(f1)\n",
        "    precision_mean = np.mean(precisions)\n",
        "    recall_mean = np.mean(recalls)\n",
        "    f1_mean = np.mean(f1Scores)\n",
        "    return f1_mean, precision_mean, recall_mean\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "IyE6cJhM3i9M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "def training_loop(\n",
        "    num_epochs,\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    dev_sents,\n",
        "    dev_labels,\n",
        "    optimizer,\n",
        "    #scheduler,\n",
        "    model,\n",
        "):\n",
        "    print(\"Training...\")\n",
        "    loss_func = torch.nn.BCELoss()\n",
        "    batches = list(zip(train_features, train_labels))\n",
        "    random.shuffle(batches)\n",
        "    for i in range(num_epochs):\n",
        "        losses = []\n",
        "        for features, labels in tqdm(batches):\n",
        "            # Empty the dynamic computation graph\n",
        "            features = features.to(device)\n",
        "            labels = labels.float()\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            #preds0, preds1, preds2, preds3, preds4, preds5, preds6, preds7, preds8, preds9, preds10, preds11, preds12, preds13, preds14, preds15, preds16, preds17, preds18, preds19  = model(features)\n",
        "            preds = model(features)\n",
        "\n",
        "            #print(preds[0].shape)\n",
        "            #featlen = preds[0].shape[0]\n",
        "            #preds_temp = torch.empty((featlen, 20), dtype=torch.float)\n",
        "            #for k in range(featlen):\n",
        "            #    for j in range(20):\n",
        "            #        preds_temp[k][j] = preds[j][k][0][0]\n",
        "            #preds = preds.squeeze(1)\n",
        "            #print(\"Preds \",preds.shape)\n",
        "            #print(\"Labels \", labels.shape)\n",
        "            #preds_temp = preds_temp.to(device)\n",
        "            #print(preds_temp.is_cuda, labels.is_cuda)\n",
        "            #print(preds_temp.shape, labels.shape)\n",
        "            #loss = loss_func(preds_temp, labels)\n",
        "            #losses = [0]*20\n",
        "            #for i in range(20):\n",
        "            #    loss1 = loss_func(preds[i],labels[:,i])\n",
        "            #    losses.append(loss1)\n",
        "            #print(preds[0].squeeze(1).squeeze(1).shape, labels[:,0].shape)\n",
        "            loss0 = loss_func(preds[0].squeeze(1).squeeze(1), labels[:,0])\n",
        "            #loss = loss_func(preds0.squeeze(1), labels)\n",
        "            loss1 = loss_func(preds[1].squeeze(1).squeeze(1), labels[:,1]) \n",
        "            loss2 = loss_func(preds[2].squeeze(1).squeeze(1), labels[:,2]) \n",
        "            loss3 = loss_func(preds[3].squeeze(1).squeeze(1), labels[:,3]) \n",
        "            loss4 = loss_func(preds[4].squeeze(1).squeeze(1), labels[:,4]) \n",
        "            loss5 = loss_func(preds[5].squeeze(1).squeeze(1), labels[:,5]) \n",
        "            loss6 = loss_func(preds[6].squeeze(1).squeeze(1), labels[:,6]) \n",
        "            loss7 = loss_func(preds[7].squeeze(1).squeeze(1), labels[:,7]) \n",
        "            loss8 = loss_func(preds[8].squeeze(1).squeeze(1), labels[:,8]) \n",
        "            loss9 = loss_func(preds[9].squeeze(1).squeeze(1), labels[:,9]) \n",
        "            loss10 = loss_func(preds[10].squeeze(1).squeeze(1), labels[:,10])\n",
        "            loss11 = loss_func(preds[11].squeeze(1).squeeze(1), labels[:,11]) \n",
        "            loss12 = loss_func(preds[12].squeeze(1).squeeze(1), labels[:,12]) \n",
        "            loss13 = loss_func(preds[13].squeeze(1).squeeze(1), labels[:,13]) \n",
        "            loss14 = loss_func(preds[14].squeeze(1).squeeze(1), labels[:,14]) \n",
        "            loss15 = loss_func(preds[15].squeeze(1).squeeze(1), labels[:,15]) \n",
        "            loss16 = loss_func(preds[16].squeeze(1).squeeze(1), labels[:,16]) \n",
        "            loss17 = loss_func(preds[17].squeeze(1).squeeze(1), labels[:,17]) \n",
        "            loss18 = loss_func(preds[18].squeeze(1).squeeze(1), labels[:,18]) \n",
        "            loss19 = loss_func(preds[19].squeeze(1).squeeze(1), labels[:,19])  \n",
        "            loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6 + loss7 + loss8 + loss9 + loss10 + loss11 + loss12 + loss13 + loss14 + loss15 + loss16 + loss17 + loss18 + loss19\n",
        "            # Backpropogate the loss through our model\n",
        "            #loss.register_hook(lambda grad: print(grad))\n",
        "            #print(model.hidden_layers[0].weight.grad)\n",
        "            #print(loss.grad)\n",
        "            loss = loss*1000\n",
        "            #print(i,model.hidden_layers[0].weight)\n",
        "            loss.backward()\n",
        "            #print(model.hidden_layers[0].weight.grad)\n",
        "            #print(loss.grad)\n",
        "            optimizer.step()\n",
        "            #print(\"After\",i, model.hidden_layers[0].weight)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "        print(f\"epoch {i}, loss: {np.sum(losses)/len(losses)}\")\n",
        "        # Estimate the f1 score for the development set\n",
        "        print(\"Evaluating dev...\")\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
        "            sents = sents.to(device)\n",
        "            pred = predict(model, sents)\n",
        "            all_preds.extend(pred)\n",
        "            all_labels.extend(list(labels))\n",
        "        # #print(range(len(set(train_labels))))\n",
        "\n",
        "        dev_f1, dev_P, dev_R = f1Score_multiLabel(all_preds, all_labels)\n",
        "        print(f\"Dev F1 {dev_f1},  Dev Precision {dev_P}, Dev Recall {dev_R}\")\n",
        "        # # #scheduler.step()\n",
        "        #print(optimizer)\n",
        "    # Return the trained model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# from tqdm import tqdm_notebook as tqdm\n",
        "# def training_loop(\n",
        "#     num_epochs,\n",
        "#     train_features,\n",
        "#     train_labels,\n",
        "#     dev_sents,\n",
        "#     dev_labels,\n",
        "#     optimizer,\n",
        "#     #scheduler,\n",
        "#     model,\n",
        "# ):\n",
        "#     print(\"Training...\")\n",
        "#     loss_func = torch.nn.BCELoss()\n",
        "#     batches = list(zip(train_features, train_labels))\n",
        "#     random.shuffle(batches)\n",
        "#     for i in range(num_epochs):\n",
        "#         losses = []\n",
        "#         for features, labels in tqdm(batches):\n",
        "#             # Empty the dynamic computation graph\n",
        "#             features = features.to(device)\n",
        "#             labels = labels.float()\n",
        "#             labels = labels.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             #preds0, preds1, preds2, preds3, preds4, preds5, preds6, preds7, preds8, preds9, preds10, preds11, preds12, preds13, preds14, preds15, preds16, preds17, preds18, preds19  = model(features)\n",
        "#             preds = model(features)\n",
        "\n",
        "#             #print(preds[0].shape)\n",
        "#             #featlen = preds[0].shape[0]\n",
        "#             #preds_temp = torch.empty((featlen, 20), dtype=torch.float)\n",
        "#             #for k in range(featlen):\n",
        "#             #    for j in range(20):\n",
        "#             #        preds_temp[k][j] = preds[j][k][0][0]\n",
        "#             #preds = preds.squeeze(1)\n",
        "#             #print(\"Preds \",preds.shape)\n",
        "#             #print(\"Labels \", labels.shape)\n",
        "#             #preds_temp = preds_temp.to(device)\n",
        "#             #print(preds_temp.is_cuda, labels.is_cuda)\n",
        "#             #print(preds_temp.shape, labels.shape)\n",
        "#             #loss = loss_func(preds_temp, labels)\n",
        "#             #losses = [0]*20\n",
        "#             #for i in range(20):\n",
        "#             #    loss1 = loss_func(preds[i],labels[:,i])\n",
        "#             #    losses.append(loss1)\n",
        "#             #print(preds[0].squeeze(1).squeeze(1).shape, labels[:,0].shape)\n",
        "#             loss0 = loss_func(preds[0].squeeze(1).squeeze(1), labels[:,0])\n",
        "#             #loss = loss_func(preds0.squeeze(1), labels)\n",
        "#             loss1 = loss_func(preds[1].squeeze(1).squeeze(1), labels[:,1]) \n",
        "#             loss2 = loss_func(preds[2].squeeze(1).squeeze(1), labels[:,2]) \n",
        "#             loss3 = loss_func(preds[3].squeeze(1).squeeze(1), labels[:,3]) \n",
        "#             loss4 = loss_func(preds[4].squeeze(1).squeeze(1), labels[:,4]) \n",
        "#             loss5 = loss_func(preds[5].squeeze(1).squeeze(1), labels[:,5]) \n",
        "#             loss6 = loss_func(preds[6].squeeze(1).squeeze(1), labels[:,6]) \n",
        "#             loss7 = loss_func(preds[7].squeeze(1).squeeze(1), labels[:,7]) \n",
        "#             loss8 = loss_func(preds[8].squeeze(1).squeeze(1), labels[:,8]) \n",
        "#             loss9 = loss_func(preds[9].squeeze(1).squeeze(1), labels[:,9]) \n",
        "#             loss10 = loss_func(preds[10].squeeze(1).squeeze(1), labels[:,10])\n",
        "#             loss11 = loss_func(preds[11].squeeze(1).squeeze(1), labels[:,11]) \n",
        "#             loss12 = loss_func(preds[12].squeeze(1).squeeze(1), labels[:,12]) \n",
        "#             loss13 = loss_func(preds[13].squeeze(1).squeeze(1), labels[:,13]) \n",
        "#             loss14 = loss_func(preds[14].squeeze(1).squeeze(1), labels[:,14]) \n",
        "#             loss15 = loss_func(preds[15].squeeze(1).squeeze(1), labels[:,15]) \n",
        "#             loss16 = loss_func(preds[16].squeeze(1).squeeze(1), labels[:,16]) \n",
        "#             loss17 = loss_func(preds[17].squeeze(1).squeeze(1), labels[:,17]) \n",
        "#             loss18 = loss_func(preds[18].squeeze(1).squeeze(1), labels[:,18]) \n",
        "#             loss19 = loss_func(preds[19].squeeze(1).squeeze(1), labels[:,19])  \n",
        "#             loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6 + loss7 + loss8 + loss9 + loss10 + loss11 + loss12 + loss13 + loss14 + loss15 + loss16 + loss17 + loss18 + loss19\n",
        "#             # Backpropogate the loss through our model\n",
        "#             #loss.register_hook(lambda grad: print(grad))\n",
        "#             #print(model.hidden_layers[0].weight.grad)\n",
        "#             #print(loss.grad)\n",
        "#             loss = loss*1000\n",
        "#             #print(i,model.hidden_layers[0].weight)\n",
        "#             loss.backward()\n",
        "#             #print(model.hidden_layers[0].weight.grad)\n",
        "#             #print(loss.grad)\n",
        "#             optimizer.step()\n",
        "#             #print(\"After\",i, model.hidden_layers[0].weight)\n",
        "#             losses.append(loss.item())\n",
        "        \n",
        "#         print(f\"epoch {i}, loss: {np.sum(losses)/len(losses)}\")\n",
        "#         # Estimate the f1 score for the development set\n",
        "#         print(\"Evaluating dev...\")\n",
        "#         all_preds = []\n",
        "#         all_labels = []\n",
        "#         for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
        "#             sents = sents.to(device)\n",
        "#             pred = predict(model, sents)\n",
        "#             all_preds.extend(pred)\n",
        "#             all_labels.extend(list(labels))\n",
        "#         # #print(range(len(set(train_labels))))\n",
        "\n",
        "#         dev_f1, dev_P, dev_R = f1Score_multiLabel(all_preds, all_labels)\n",
        "#         print(f\"Dev F1 {dev_f1},  Dev Precision {dev_P}, Dev Recall {dev_R}\")\n",
        "#         # # #scheduler.step()\n",
        "#         #print(optimizer)\n",
        "#     # Return the trained model\n",
        "#     return model"
      ],
      "metadata": {
        "id": "N6NQ1Vpb3z2k"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liqb8oDn4fNx",
        "outputId": "db4ad508-ab04-47b6-eed2-aea0408fd627",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "# TODO: Find a good learning rate\n",
        "LR = 1e-5\n",
        "\n",
        "possible_labels = 20\n",
        "model = NLIClassifier(output_size=possible_labels, hidden_size=512)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(), LR)"
      ],
      "metadata": {
        "id": "lwia5LTU7xJ2"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "f31ed555a96c41d5aaa68caabc76866d",
            "970e32a86c63433ea101472fd43f92b8",
            "0c8d79fd83b6446e87b85aa5ee92bfe5",
            "29f158b67f894d03ac8b7491eb03504e",
            "d06a135d961d4969abaf1cad88e6bf66",
            "0f0873e2b35c4229871cff349515244b",
            "9991d19282cd400a8c1f35cf1a8b4708",
            "df1ca5011c0f49b8a020bcaa5fa869bf",
            "ecc69437241c4b458d261f4d424a6d82",
            "dffb7f8565c345a7b55c7d4f8023650c",
            "ea99c0f55fad447ca7f38e3522f1d640",
            "a7d1e6f194cd49e68de6e68990327f1f",
            "81845f59726947c19d9c2867babd7b42",
            "4aa54fe2353945ba8cd0063f309b7ae5",
            "eb19be658f994fedb355ffd515c67e3d",
            "8215eb94142b4e38b02295e692beacb8",
            "f5486594d52544f18652f1c2ed40e431",
            "d3b004739a104c1a8788a50a1c6820c2",
            "115afbec1bf14fb88d48b741aac2b556",
            "a937ca14107e4473b5598843a3cc70f0",
            "8c24bf5994e048eb9b1cee0440e34660",
            "ade0149ca54f434f993dc5205c0bcf7b"
          ]
        },
        "id": "Y0ThGzo64w64",
        "outputId": "f06fa622-ba30-4698-93a3-1166d4f554ea",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-213-1dbb0273a955>:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for features, labels in tqdm(batches):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/75 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f31ed555a96c41d5aaa68caabc76866d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss: 10096.968411458334\n",
            "Evaluating dev...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-213-1dbb0273a955>:86: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7d1e6f194cd49e68de6e68990327f1f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model =training_loop(\n",
        "    epochs,\n",
        "    train_input_batches,\n",
        "    train_label_batches,\n",
        "    val_input_batches,\n",
        "    val_label_batches,\n",
        "    optimizer,\n",
        "    #scheduler,\n",
        "    model,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = (1,2,3)\n",
        "a[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xyarnd1xoP-L",
        "outputId": "2ec625d5-b711-4f83-f8d4-3beb6e7bc754"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "ee66bd0ea5e44abe8547960d2b953dfd",
            "6794b7344ca74ea79e27d62cc2b731e7",
            "46e9ef759c5a4076af228ca215de5f8a",
            "cab1a59fad4042279965587e565d316a",
            "be051014d67743fbb9d84f7079ef0cd2",
            "7e9aaae0e19745f2ac45d9d45bdf21c0",
            "ab3d4f183f6443e88b4a4b78e5bbe1e2",
            "ad16b7f28a1a45e689ec8389b1713bd4",
            "d775d856f78f429fb3235667a49d23e2",
            "3d49b2fe098646e6b8dd092a1e254e4f",
            "9ba695ce3a904eb2a3e602c5cf5e538a"
          ]
        },
        "id": "_4203j4811Mw",
        "outputId": "2e954a67-aa63-41c2-8880-0f849bb20b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating dev...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-124-ee7ac79b3e67>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for sents, labels in tqdm(zip(val_input_batches, val_label_batches), total=len(val_input_batches)):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee66bd0ea5e44abe8547960d2b953dfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[140, 114, 2, 0, 34, 0, 5, 34, 31, 37, 185, 0, 91, 0, 13, 0, 0, 0, 10, 50] [500, 500, 12, 9, 153, 0, 500, 500, 500, 500, 500, 0, 394, 0, 493, 0, 1, 0, 82, 482] [140, 114, 53, 6, 108, 22, 5, 34, 31, 37, 185, 145, 105, 62, 13, 39, 171, 26, 99, 52]\n",
            "0.28 1.0 0\n",
            "0.228 1.0 1\n",
            "0.16666666666666666 0.03773584905660377 2\n",
            "0.0 0.0 3\n",
            "0.2222222222222222 0.3148148148148148 4\n",
            "0.2222222222222222 0.0 5\n",
            "0.01 1.0 6\n",
            "0.068 1.0 7\n",
            "0.062 1.0 8\n",
            "0.074 1.0 9\n",
            "0.37 1.0 10\n",
            "0.37 0.0 11\n",
            "0.23096446700507614 0.8666666666666667 12\n",
            "0.23096446700507614 0.0 13\n",
            "0.02636916835699797 1.0 14\n",
            "0.02636916835699797 0.0 15\n",
            "0.0 0.0 16\n",
            "0.0 0.0 17\n",
            "0.12195121951219512 0.10101010101010101 18\n",
            "0.1037344398340249 0.9615384615384616 19\n",
            "Dev F1 0.5140882946543324,  Dev Precision 0.140673202059074, Dev Recall 0.5140882946543324\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating dev...\")\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "for sents, labels in tqdm(zip(val_input_batches, val_label_batches), total=len(val_input_batches)):\n",
        "    pred = predict(model, sents)\n",
        "    all_preds.extend(pred)\n",
        "    all_labels.extend(list(labels.numpy()))\n",
        "# #print(range(len(set(train_labels))))\n",
        "\n",
        "dev_f1, dev_P, dev_R = f1Score_multiLabel(all_preds, all_labels)\n",
        "print(f\"Dev F1 {dev_f1},  Dev Precision {dev_P}, Dev Recall {dev_R}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNtkjU5-49vL",
        "outputId": "48a3cf8c-a04f-4a12-8343-2362f1c02844"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(all_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeRlExU2w8Lp"
      },
      "outputs": [],
      "source": [
        "def f1Score_multiLabel(preds, labels):\n",
        "    nLabels = 20\n",
        "    relevants = [0]*20\n",
        "    positives = [0]*20\n",
        "    truePositives = [0]*20\n",
        "    for i in range(len(preds)):\n",
        "        for j in range(nLabels):\n",
        "            if(preds[i][j]==1):\n",
        "                positives[j] += 1\n",
        "                if(labels[i][j]==1):\n",
        "                    truePositives[j] += 1\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "        for j in range(nLabels):\n",
        "            if(labels[i][j]==1):\n",
        "                relevants[j] += 1\n",
        "    \n",
        "    precisions = []*nLabels\n",
        "    recalls = []*nLabels\n",
        "    f1Scores = []*nLabels\n",
        "    precision =0\n",
        "    recall = 0\n",
        "    f1 = 0\n",
        "    print(truePositives, positives, relevants)\n",
        "    for i in range(nLabels):\n",
        "        if(positives[i]>0):\n",
        "            precision = truePositives[i]/positives[i]\n",
        "        precisions.append(precision)\n",
        "        if(relevants[i]>0):\n",
        "            recall = truePositives[i]/relevants[i]\n",
        "        recalls.append(recall)\n",
        "        print(precision,recall,i)\n",
        "        if(positives[i]>0 and relevants[i]>0):\n",
        "            f1 = 2 * precision * recall / (precision + recall)\n",
        "        f1Scores.append(f1)\n",
        "    precision_mean = np.mean(precisions)\n",
        "    recall_mean = np.mean(recalls)\n",
        "    f1_mean = np.mean(recalls)\n",
        "    return f1_mean, precision_mean, recall_mean\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxPeUYO6xlip",
        "outputId": "796fcdcf-7394-4e6c-9b2c-8c0950de84b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_label_batches[0][0][10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B57iw_741yk",
        "outputId": "1e371b64-30dd-48dc-c172-2d3b05ae22a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0589,  0.2156, -1.1975, -0.6472, -0.6455], requires_grad=True) tensor([0.2369, 0.0586, 0.4756, 0.1383, 0.0906])\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(5, requires_grad=True)\n",
        "#target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "target = torch.randn(5).softmax(dim=0)\n",
        "print(input, target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXnrg3Ph5QRW",
        "outputId": "e99baf0f-73ea-4174-e109-046a835b9387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.9810, grad_fn=<DivBackward1>)\n"
          ]
        }
      ],
      "source": [
        "output = loss(input, target)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRrI3eptcFJd",
        "outputId": "a949b623-97ad-43b1-892f-e8be417181c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1021,  0.0259,     nan, -0.8798,  0.3323]], grad_fn=<LogBackward0>)"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.log(input)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWA-g2dDch-j",
        "outputId": "f6767fcf-20c9-4d59-96f3-bdd7e0ba8d61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0537,  0.0045,     nan, -0.0182,  0.0056]], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a*target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMUDSzOFctdG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwDuG-5pWVVs",
        "outputId": "142e8a75-a001-4973-978f-0e8444b5bd35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5930, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "target = torch.empty(1, dtype=torch.long)\n",
        "target[0] = 1\n",
        "output = loss(input, target)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4XKS621WmjX",
        "outputId": "6326ce58-1ee0-4e31-f68f-2c3793ab7e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, loss: 10.0\n"
          ]
        }
      ],
      "source": [
        "i = 1\n",
        "losses=[10]\n",
        "print(f\"epoch {i}, loss: {np.sum(losses)/len(losses)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIzpG6eFW6pb",
        "outputId": "399968fe-b790-4094-c9ac-efe21a044665"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDPOLhR_XAdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f909351-0564-4553-bfe5-c1711d6d09b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4920, -0.2966, -0.1552,  0.1589,  0.5150],\n",
              "        [-0.3642,  0.2618,  0.9885,  0.9222, -0.0693]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "from torch import nn\n",
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(2, 5, requires_grad=True)\n",
        "input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = input[:,2]\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H-Loez5VcQ5",
        "outputId": "0a3dfb98-85bd-4c43-d582-ab6c10fa7971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1552,  0.9885], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8NS42NbVqGD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d49b2fe098646e6b8dd092a1e254e4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e9ef759c5a4076af228ca215de5f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad16b7f28a1a45e689ec8389b1713bd4",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d775d856f78f429fb3235667a49d23e2",
            "value": 8
          }
        },
        "6794b7344ca74ea79e27d62cc2b731e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e9aaae0e19745f2ac45d9d45bdf21c0",
            "placeholder": "​",
            "style": "IPY_MODEL_ab3d4f183f6443e88b4a4b78e5bbe1e2",
            "value": "100%"
          }
        },
        "7e9aaae0e19745f2ac45d9d45bdf21c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba695ce3a904eb2a3e602c5cf5e538a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab3d4f183f6443e88b4a4b78e5bbe1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad16b7f28a1a45e689ec8389b1713bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be051014d67743fbb9d84f7079ef0cd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cab1a59fad4042279965587e565d316a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d49b2fe098646e6b8dd092a1e254e4f",
            "placeholder": "​",
            "style": "IPY_MODEL_9ba695ce3a904eb2a3e602c5cf5e538a",
            "value": " 8/8 [02:22&lt;00:00, 22.09s/it]"
          }
        },
        "d775d856f78f429fb3235667a49d23e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee66bd0ea5e44abe8547960d2b953dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6794b7344ca74ea79e27d62cc2b731e7",
              "IPY_MODEL_46e9ef759c5a4076af228ca215de5f8a",
              "IPY_MODEL_cab1a59fad4042279965587e565d316a"
            ],
            "layout": "IPY_MODEL_be051014d67743fbb9d84f7079ef0cd2"
          }
        },
        "f31ed555a96c41d5aaa68caabc76866d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_970e32a86c63433ea101472fd43f92b8",
              "IPY_MODEL_0c8d79fd83b6446e87b85aa5ee92bfe5",
              "IPY_MODEL_29f158b67f894d03ac8b7491eb03504e"
            ],
            "layout": "IPY_MODEL_d06a135d961d4969abaf1cad88e6bf66"
          }
        },
        "970e32a86c63433ea101472fd43f92b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f0873e2b35c4229871cff349515244b",
            "placeholder": "​",
            "style": "IPY_MODEL_9991d19282cd400a8c1f35cf1a8b4708",
            "value": "100%"
          }
        },
        "0c8d79fd83b6446e87b85aa5ee92bfe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df1ca5011c0f49b8a020bcaa5fa869bf",
            "max": 75,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecc69437241c4b458d261f4d424a6d82",
            "value": 75
          }
        },
        "29f158b67f894d03ac8b7491eb03504e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dffb7f8565c345a7b55c7d4f8023650c",
            "placeholder": "​",
            "style": "IPY_MODEL_ea99c0f55fad447ca7f38e3522f1d640",
            "value": " 75/75 [00:18&lt;00:00,  3.93it/s]"
          }
        },
        "d06a135d961d4969abaf1cad88e6bf66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0873e2b35c4229871cff349515244b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9991d19282cd400a8c1f35cf1a8b4708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df1ca5011c0f49b8a020bcaa5fa869bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc69437241c4b458d261f4d424a6d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dffb7f8565c345a7b55c7d4f8023650c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea99c0f55fad447ca7f38e3522f1d640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7d1e6f194cd49e68de6e68990327f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81845f59726947c19d9c2867babd7b42",
              "IPY_MODEL_4aa54fe2353945ba8cd0063f309b7ae5",
              "IPY_MODEL_eb19be658f994fedb355ffd515c67e3d"
            ],
            "layout": "IPY_MODEL_8215eb94142b4e38b02295e692beacb8"
          }
        },
        "81845f59726947c19d9c2867babd7b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5486594d52544f18652f1c2ed40e431",
            "placeholder": "​",
            "style": "IPY_MODEL_d3b004739a104c1a8788a50a1c6820c2",
            "value": "100%"
          }
        },
        "4aa54fe2353945ba8cd0063f309b7ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_115afbec1bf14fb88d48b741aac2b556",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a937ca14107e4473b5598843a3cc70f0",
            "value": 8
          }
        },
        "eb19be658f994fedb355ffd515c67e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c24bf5994e048eb9b1cee0440e34660",
            "placeholder": "​",
            "style": "IPY_MODEL_ade0149ca54f434f993dc5205c0bcf7b",
            "value": " 8/8 [00:01&lt;00:00,  4.94it/s]"
          }
        },
        "8215eb94142b4e38b02295e692beacb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5486594d52544f18652f1c2ed40e431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3b004739a104c1a8788a50a1c6820c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "115afbec1bf14fb88d48b741aac2b556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a937ca14107e4473b5598843a3cc70f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c24bf5994e048eb9b1cee0440e34660": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ade0149ca54f434f993dc5205c0bcf7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}